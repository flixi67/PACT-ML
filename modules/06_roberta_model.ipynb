{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# just for Google Colab Notebook\n",
    "!pip install transformers datasets evaluate scikit-learn scikit-multilearn --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Stratification k-Fold CV\n",
    "\n",
    "Just like in the other models for comparable results. Applied a bit differently than in Scikit-Learn framework =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import IterativeStratification\n",
    "\n",
    "# Set up Iterative Stratification\n",
    "n_splits = 5\n",
    "stratifier = IterativeStratification(n_splits=n_splits, order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import f1_score, multilabel_confusion_matrix, classification_report, precision_score, recall_score, accuracy_score\n",
    "\n",
    "df = pd.read_csv(\"data/merged_data.csv\")\n",
    "\n",
    "target_categories = [\n",
    "    \"PoliceReform\",\n",
    "    \"Operations_PatrolsInterventions\",\n",
    "    \"StateAdministration\",\n",
    "    \"RefugeeAssistance\",\n",
    "    \"ElectionAssistance\",\n",
    "    \"LegalReform\",\n",
    "    \"CivilSocietyAssistance\"\n",
    "]\n",
    "\n",
    "X = np.array(df[\"paragraph\"].tolist())\n",
    "Y = np.array(df[target_categories].fillna(False).astype(int).values)\n",
    "\n",
    "# Tokenizer & Encoding\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "all_folds = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(stratifier.split(X, Y)):\n",
    "    print(f\"\\nüìÇ FOLD {fold + 1}\")\n",
    "\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = Y[train_idx], Y[val_idx]\n",
    "\n",
    "    train_dataset = Dataset.from_dict({\"text\": X_train.tolist(), \"labels\": y_train.tolist()})\n",
    "    val_dataset = Dataset.from_dict({\"text\": X_val.tolist(), \"labels\": y_val.tolist()})\n",
    "\n",
    "    def tokenize(example):\n",
    "        tokenized = tokenizer(\n",
    "            example[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256\n",
    "        )\n",
    "        # Cast labels to float32 for BCEWithLogitsLoss\n",
    "        tokenized[\"labels\"] = [float(x) for x in example[\"labels\"]]\n",
    "        return tokenized\n",
    "    \n",
    "    train_dataset = train_dataset.map(tokenize)\n",
    "    val_dataset = val_dataset.map(tokenize)\n",
    "\n",
    "    num_labels = Y.shape[1]\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "        \"xlm-roberta-base\",\n",
    "        num_labels=num_labels,\n",
    "        problem_type=\"multi_label_classification\"\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/fold_{fold+1}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=4,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        disable_tqdm=False,\n",
    "        logging_dir=f\"./logs/fold_{fold+1}\",\n",
    "        save_strategy=\"no\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Predict\n",
    "    preds = trainer.predict(val_dataset)\n",
    "    probs = torch.sigmoid(torch.tensor(preds.predictions)).numpy()\n",
    "    Y_pred = (probs > 0.5).astype(int)\n",
    "    Y_true = preds.label_ids\n",
    "\n",
    "    # Confusion Metrics\n",
    "    cm = multilabel_confusion_matrix(Y_true, Y_pred)\n",
    "\n",
    "    rows = []\n",
    "    for i, label in enumerate(label_names):\n",
    "        tn, fp, fn, tp = cm[i].ravel()\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "        rows.append({\n",
    "            'Model': 'XLM-RoBERTa',\n",
    "            'Fold': fold + 1,\n",
    "            'Label': label,\n",
    "            'F1_micro': f1_score(Y_true, Y_pred, average='micro'),\n",
    "            'F1_macro': f1_score(Y_true, Y_pred, average='macro'),\n",
    "            'FPR': fpr,\n",
    "            'FNR': fnr,\n",
    "            'TPR': tpr,\n",
    "            'TNR': tnr,\n",
    "            'TP': tp,\n",
    "            'FP': fp,\n",
    "            'FN': fn,\n",
    "            'TN': tn\n",
    "        })\n",
    "\n",
    "    print(classification_report(Y_true, Y_pred, target_names=label_names, zero_division=0))\n",
    "    \n",
    "    fold_df = pd.DataFrame(rows)\n",
    "    all_folds.append(fold_df)\n",
    "\n",
    "final_results_df = pd.concat(all_folds, ignore_index=True)\n",
    "\n",
    "# Summary per label\n",
    "summary_df = final_results_df.groupby(['Model', 'Label']).agg({\n",
    "    'F1_micro': 'mean',\n",
    "    'F1_macro': 'mean',\n",
    "    'FPR': 'mean',\n",
    "    'FNR': 'mean',\n",
    "    'TPR': 'mean',\n",
    "    'TNR': 'mean',\n",
    "}).reset_index()\n",
    "\n",
    "summary_df.to_csv(\"model_performance_summary_bert.csv\", index=False)\n",
    "print(\"üìÅ Results saved as model_performance_summary_bert.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./multilabel_model_xlmr\")\n",
    "tokenizer.save_pretrained(\"./multilabel_model_xlmr\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
