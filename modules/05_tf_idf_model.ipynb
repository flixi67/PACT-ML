{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced BoW model (TF-IDF vectorization)\n",
    "\n",
    "TF-IDF model using a distinctive terms vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\felix\\Documents\\GIT\\Hertie\\PACT-ML\\modules\n",
      "Added to path: c:\\Users\\felix\\Documents\\GIT\\Hertie\\PACT-ML\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Add the parent directory to the path if needed\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "print(f\"Added to path: {parent_dir}\")\n",
    "\n",
    "# set global seeds for reproducability\n",
    "random.seed(161)\n",
    "np.random.seed(161)\n",
    "os.environ['PYTHONHASHSEED'] = '161'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyreadr import read_r\n",
    "from modules.helpers.validity_check import fuzzy_match_report_key\n",
    "\n",
    "para_data = pd.read_csv(\"..\\data\\PACT_paragraphs_training.csv\")\n",
    "\n",
    "report_data = pd.read_csv(\"..\\data\\paragraphs.csv\", sep=';')\n",
    "\n",
    "report_data[\"matchingKey\"] = report_data[\"report_namePKO\"].str.replace('/', '_')\n",
    "\n",
    "# reduced data to 7 target categories with most codings\n",
    "target_categories = [\n",
    "    \"PoliceReform\",\n",
    "    \"Operations_PatrolsInterventions\",\n",
    "    \"StateAdministration\",\n",
    "    \"RefugeeAssistance\",\n",
    "    \"ElectionAssistance\",\n",
    "    \"LegalReform\",\n",
    "    \"CivilSocietyAssistance\"\n",
    "]\n",
    "\n",
    "report_data = report_data[[\"matchingKey\", \"paragraphNumber\"] + target_categories]\n",
    "\n",
    "report_data[target_categories] = report_data[target_categories].map(lambda x: isinstance(x, str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of merged data: (6029, 12)\n",
      "Number of features for TF-IDF: 6144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "merged_data = pd.merge(\n",
    "    para_data,\n",
    "    report_data,\n",
    "    on=[\"matchingKey\", \"paragraphNumber\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "merged_data.to_csv(\"../data/merged_data.csv\", index=False)\n",
    "\n",
    "# Check the shape of the merged data\n",
    "print(f\"Shape of merged data: {merged_data.shape}\")\n",
    "\n",
    "# vectorizer = CountVectorizer(stop_words='english', min_df=5, max_df=0.95)\n",
    "# X = vectorizer.fit_transform(merged_data[\"paragraph\"])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.95)\n",
    "X = tfidf_vectorizer.fit_transform(merged_data['paragraph'])\n",
    "\n",
    "# df_bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_tfidf = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "# print(f\"Number of features for Count Vectorizer: {df_bow.shape[1]}\")\n",
    "print(f\"Number of features for TF-IDF: {df_tfidf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Stratification K-fold CV\n",
    "\n",
    "To make the most of our limited multi-label data, we use Iterative Stratification K-fold CV across all our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-7>:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "\n",
    "Y = merged_data[target_categories].fillna(False).astype(int).values\n",
    "\n",
    "# Set up Iterative Stratification\n",
    "n_splits = 5\n",
    "stratifier = IterativeStratification(n_splits=n_splits, order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Model + Random Forest using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Logistic Regression ---\n",
      "Average F1 (micro): 0.3564\n",
      "Average F1 (macro): 0.3365\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.72      0.28      0.40       105\n",
      "Operations_PatrolsInterventions       0.83      0.33      0.48        30\n",
      "            StateAdministration       0.78      0.16      0.26        44\n",
      "              RefugeeAssistance       0.43      0.15      0.22        20\n",
      "             ElectionAssistance       0.75      0.27      0.40        22\n",
      "                    LegalReform       1.00      0.15      0.27        26\n",
      "         CivilSocietyAssistance       0.47      0.25      0.33        32\n",
      "\n",
      "                      micro avg       0.69      0.24      0.36       279\n",
      "                      macro avg       0.71      0.23      0.34       279\n",
      "                   weighted avg       0.72      0.24      0.35       279\n",
      "                    samples avg       0.05      0.04      0.05       279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.3853\n",
      "Average F1 (macro): 0.3367\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.74      0.40      0.52       105\n",
      "Operations_PatrolsInterventions       0.67      0.40      0.50        30\n",
      "            StateAdministration       0.92      0.28      0.43        43\n",
      "              RefugeeAssistance       1.00      0.05      0.09        21\n",
      "             ElectionAssistance       0.67      0.45      0.54        22\n",
      "                    LegalReform       0.40      0.08      0.13        26\n",
      "         CivilSocietyAssistance       0.38      0.09      0.15        32\n",
      "\n",
      "                      micro avg       0.70      0.29      0.41       279\n",
      "                      macro avg       0.68      0.25      0.34       279\n",
      "                   weighted avg       0.70      0.29      0.39       279\n",
      "                    samples avg       0.06      0.06      0.06       279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.3962\n",
      "Average F1 (macro): 0.3384\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.65      0.42      0.51       105\n",
      "Operations_PatrolsInterventions       1.00      0.47      0.64        30\n",
      "            StateAdministration       0.86      0.27      0.41        44\n",
      "              RefugeeAssistance       0.50      0.20      0.29        20\n",
      "             ElectionAssistance       0.40      0.09      0.15        22\n",
      "                    LegalReform       0.50      0.08      0.14        25\n",
      "         CivilSocietyAssistance       0.83      0.16      0.26        32\n",
      "\n",
      "                      micro avg       0.70      0.30      0.42       278\n",
      "                      macro avg       0.68      0.24      0.34       278\n",
      "                   weighted avg       0.70      0.30      0.40       278\n",
      "                    samples avg       0.07      0.06      0.06       278\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.4058\n",
      "Average F1 (macro): 0.3501\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.70      0.43      0.53       105\n",
      "Operations_PatrolsInterventions       0.86      0.40      0.55        30\n",
      "            StateAdministration       0.64      0.20      0.31        44\n",
      "              RefugeeAssistance       0.67      0.29      0.40        21\n",
      "             ElectionAssistance       0.60      0.26      0.36        23\n",
      "                    LegalReform       0.60      0.12      0.19        26\n",
      "         CivilSocietyAssistance       1.00      0.21      0.35        33\n",
      "\n",
      "                      micro avg       0.72      0.31      0.43       282\n",
      "                      macro avg       0.72      0.27      0.39       282\n",
      "                   weighted avg       0.72      0.31      0.42       282\n",
      "                    samples avg       0.07      0.06      0.06       282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.3970\n",
      "Average F1 (macro): 0.3436\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.57      0.33      0.42       105\n",
      "Operations_PatrolsInterventions       0.80      0.39      0.52        31\n",
      "            StateAdministration       0.73      0.18      0.29        44\n",
      "              RefugeeAssistance       0.60      0.14      0.23        21\n",
      "             ElectionAssistance       0.75      0.13      0.22        23\n",
      "                    LegalReform       0.43      0.12      0.18        26\n",
      "         CivilSocietyAssistance       0.62      0.25      0.36        32\n",
      "\n",
      "                      micro avg       0.62      0.26      0.36       282\n",
      "                      macro avg       0.64      0.22      0.32       282\n",
      "                   weighted avg       0.63      0.26      0.35       282\n",
      "                    samples avg       0.05      0.05      0.05       282\n",
      "\n",
      "\n",
      "--- Balanced Logistic Regression ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.3678\n",
      "Average F1 (macro): 0.3494\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.41      0.70      0.52       105\n",
      "Operations_PatrolsInterventions       0.23      0.71      0.35        31\n",
      "            StateAdministration       0.16      0.55      0.25        44\n",
      "              RefugeeAssistance       0.23      0.90      0.37        21\n",
      "             ElectionAssistance       0.21      0.86      0.34        22\n",
      "                    LegalReform       0.17      0.69      0.27        26\n",
      "         CivilSocietyAssistance       0.22      0.78      0.35        32\n",
      "\n",
      "                      micro avg       0.25      0.72      0.37       281\n",
      "                      macro avg       0.23      0.74      0.35       281\n",
      "                   weighted avg       0.28      0.72      0.39       281\n",
      "                    samples avg       0.11      0.13      0.11       281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.3823\n",
      "Average F1 (macro): 0.3638\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.41      0.80      0.55       105\n",
      "Operations_PatrolsInterventions       0.23      0.83      0.36        30\n",
      "            StateAdministration       0.18      0.70      0.28        43\n",
      "              RefugeeAssistance       0.24      0.85      0.37        20\n",
      "             ElectionAssistance       0.23      0.86      0.37        22\n",
      "                    LegalReform       0.20      0.92      0.33        26\n",
      "         CivilSocietyAssistance       0.26      0.88      0.40        32\n",
      "\n",
      "                      micro avg       0.26      0.82      0.40       278\n",
      "                      macro avg       0.25      0.83      0.38       278\n",
      "                   weighted avg       0.29      0.82      0.42       278\n",
      "                    samples avg       0.12      0.15      0.13       278\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.3804\n",
      "Average F1 (macro): 0.3623\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.37      0.74      0.49       105\n",
      "Operations_PatrolsInterventions       0.21      0.83      0.34        30\n",
      "            StateAdministration       0.20      0.70      0.32        44\n",
      "              RefugeeAssistance       0.22      0.65      0.33        20\n",
      "             ElectionAssistance       0.26      0.91      0.40        22\n",
      "                    LegalReform       0.15      0.77      0.26        26\n",
      "         CivilSocietyAssistance       0.24      0.94      0.38        32\n",
      "\n",
      "                      micro avg       0.25      0.78      0.38       279\n",
      "                      macro avg       0.24      0.79      0.36       279\n",
      "                   weighted avg       0.27      0.78      0.39       279\n",
      "                    samples avg       0.12      0.14      0.12       279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.3870\n",
      "Average F1 (macro): 0.3683\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.42      0.76      0.54       105\n",
      "Operations_PatrolsInterventions       0.26      0.80      0.39        30\n",
      "            StateAdministration       0.19      0.50      0.28        44\n",
      "              RefugeeAssistance       0.25      0.86      0.38        21\n",
      "             ElectionAssistance       0.28      0.87      0.43        23\n",
      "                    LegalReform       0.16      0.68      0.26        25\n",
      "         CivilSocietyAssistance       0.28      0.85      0.42        33\n",
      "\n",
      "                      micro avg       0.28      0.74      0.41       281\n",
      "                      macro avg       0.26      0.76      0.39       281\n",
      "                   weighted avg       0.30      0.74      0.42       281\n",
      "                    samples avg       0.12      0.14      0.12       281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.3876\n",
      "Average F1 (macro): 0.3678\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.40      0.78      0.53       105\n",
      "Operations_PatrolsInterventions       0.22      0.80      0.34        30\n",
      "            StateAdministration       0.22      0.68      0.33        44\n",
      "              RefugeeAssistance       0.23      0.86      0.36        21\n",
      "             ElectionAssistance       0.20      0.83      0.32        23\n",
      "                    LegalReform       0.20      0.88      0.33        26\n",
      "         CivilSocietyAssistance       0.22      0.78      0.35        32\n",
      "\n",
      "                      micro avg       0.26      0.79      0.39       281\n",
      "                      macro avg       0.24      0.80      0.37       281\n",
      "                   weighted avg       0.28      0.79      0.41       281\n",
      "                    samples avg       0.12      0.15      0.13       281\n",
      "\n",
      "\n",
      "--- Random Forest ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.1694\n",
      "Average F1 (macro): 0.1287\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.92      0.11      0.20       105\n",
      "Operations_PatrolsInterventions       1.00      0.20      0.33        30\n",
      "            StateAdministration       0.88      0.16      0.27        44\n",
      "              RefugeeAssistance       1.00      0.05      0.10        20\n",
      "             ElectionAssistance       0.00      0.00      0.00        22\n",
      "                    LegalReform       0.00      0.00      0.00        26\n",
      "         CivilSocietyAssistance       0.00      0.00      0.00        32\n",
      "\n",
      "                      micro avg       0.93      0.09      0.17       279\n",
      "                      macro avg       0.54      0.07      0.13       279\n",
      "                   weighted avg       0.66      0.09      0.16       279\n",
      "                    samples avg       0.02      0.02      0.02       279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.1478\n",
      "Average F1 (macro): 0.1095\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.90      0.09      0.16       105\n",
      "Operations_PatrolsInterventions       1.00      0.13      0.24        30\n",
      "            StateAdministration       1.00      0.14      0.24        44\n",
      "              RefugeeAssistance       0.00      0.00      0.00        21\n",
      "             ElectionAssistance       0.00      0.00      0.00        23\n",
      "                    LegalReform       0.00      0.00      0.00        26\n",
      "         CivilSocietyAssistance       0.00      0.00      0.00        32\n",
      "\n",
      "                      micro avg       0.95      0.07      0.13       281\n",
      "                      macro avg       0.41      0.05      0.09       281\n",
      "                   weighted avg       0.60      0.07      0.12       281\n",
      "                    samples avg       0.01      0.01      0.01       281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.1508\n",
      "Average F1 (macro): 0.1152\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.92      0.11      0.20       105\n",
      "Operations_PatrolsInterventions       1.00      0.13      0.23        31\n",
      "            StateAdministration       1.00      0.11      0.20        44\n",
      "              RefugeeAssistance       1.00      0.05      0.09        21\n",
      "             ElectionAssistance       1.00      0.04      0.08        23\n",
      "                    LegalReform       1.00      0.04      0.08        25\n",
      "         CivilSocietyAssistance       0.00      0.00      0.00        32\n",
      "\n",
      "                      micro avg       0.96      0.09      0.16       281\n",
      "                      macro avg       0.85      0.07      0.13       281\n",
      "                   weighted avg       0.86      0.09      0.15       281\n",
      "                    samples avg       0.02      0.02      0.02       281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.1553\n",
      "Average F1 (macro): 0.1168\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.82      0.13      0.23       105\n",
      "Operations_PatrolsInterventions       1.00      0.17      0.29        30\n",
      "            StateAdministration       1.00      0.14      0.24        43\n",
      "              RefugeeAssistance       1.00      0.05      0.09        21\n",
      "             ElectionAssistance       0.00      0.00      0.00        22\n",
      "                    LegalReform       0.00      0.00      0.00        26\n",
      "         CivilSocietyAssistance       0.00      0.00      0.00        32\n",
      "\n",
      "                      micro avg       0.90      0.09      0.17       279\n",
      "                      macro avg       0.55      0.07      0.12       279\n",
      "                   weighted avg       0.65      0.09      0.16       279\n",
      "                    samples avg       0.02      0.02      0.02       279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.1529\n",
      "Average F1 (macro): 0.1160\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.92      0.10      0.19       105\n",
      "Operations_PatrolsInterventions       0.83      0.17      0.28        30\n",
      "            StateAdministration       1.00      0.09      0.17        44\n",
      "              RefugeeAssistance       0.00      0.00      0.00        20\n",
      "             ElectionAssistance       0.50      0.05      0.08        22\n",
      "                    LegalReform       0.50      0.04      0.07        26\n",
      "         CivilSocietyAssistance       0.00      0.00      0.00        33\n",
      "\n",
      "                      micro avg       0.81      0.08      0.14       280\n",
      "                      macro avg       0.54      0.06      0.11       280\n",
      "                   weighted avg       0.68      0.08      0.14       280\n",
      "                    samples avg       0.01      0.01      0.01       280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, multilabel_confusion_matrix\n",
    "\n",
    "best_params_rf = {'estimator__max_depth': None, 'estimator__min_samples_split': 2, 'estimator__n_estimators': 75}\n",
    "\n",
    "rf_params = {k.replace('estimator__', ''): v for k, v in best_params_rf.items()}\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": OneVsRestClassifier(LogisticRegression(max_iter=1000, C= 10.0, penalty='l1', solver='liblinear')),\n",
    "    \"Balanced Logistic Regression\": OneVsRestClassifier(LogisticRegression(class_weight='balanced', max_iter=1000, C= 1.0, penalty='l2', solver='liblinear')),\n",
    "    \"Random Forest\": RandomForestClassifier(**rf_params)\n",
    "}\n",
    "\n",
    "label_names = merged_data[target_categories].columns.tolist()\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "\n",
    "    # Store metrics per fold, per label\n",
    "    fold_metrics = []\n",
    "\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_idx, test_idx in stratifier.split(X, Y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "\n",
    "        Y_pred = Y_pred.toarray() if hasattr(Y_pred, 'toarray') else Y_pred\n",
    "\n",
    "        f1_micro = f1_score(Y_test, Y_pred, average='micro')  # or 'macro'\n",
    "        f1_macro = f1_score(Y_test, Y_pred, average='macro')\n",
    "        f1_scores.append({'f1_micro': f1_micro, 'f1_macro': f1_macro})\n",
    "        f1_scores_df = pd.DataFrame(f1_scores)\n",
    "\n",
    "        # Compute confusion matrices per label\n",
    "        cm = multilabel_confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "        rows = []\n",
    "\n",
    "        # Show false posotives and false negatives per label\n",
    "        for i, label in enumerate(label_names):\n",
    "            tn, fp, fn, tp = cm[i].ravel()\n",
    "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "            fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "            tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "            fold_metrics.append({\n",
    "                'Model': model_name,\n",
    "                'Fold': len(fold_metrics) // len(label_names) + 1,\n",
    "                'Label': label,\n",
    "                'F1_micro': f1_micro,\n",
    "                'F1_macro': f1_macro,\n",
    "                'FPR': fpr,\n",
    "                'FNR': fnr,\n",
    "                'TPR': tpr,\n",
    "                'TNR': tnr,\n",
    "                'TP': tp,\n",
    "                'FP': fp,\n",
    "                'FN': fn,\n",
    "                'TN': tn\n",
    "            })\n",
    "\n",
    "        # Convert per-fold metrics to DataFrame\n",
    "        fold_metrics_df = pd.DataFrame(fold_metrics)\n",
    "\n",
    "        # Aggregate over folds: mean per model and label\n",
    "        summary_df = fold_metrics_df.groupby(['Model', 'Label']).agg({\n",
    "            'F1_micro': 'mean',\n",
    "            'F1_macro': 'mean',\n",
    "            'FPR': 'mean',\n",
    "            'FNR': 'mean',\n",
    "            'TPR': 'mean',\n",
    "            'TNR': 'mean',\n",
    "        }).reset_index()\n",
    "\n",
    "        # Append summary to global results list\n",
    "        results.append(summary_df)\n",
    "\n",
    "        print(f\"Average F1 (micro): {np.mean(f1_scores_df.f1_micro):.4f}\")\n",
    "        print(f\"Average F1 (macro): {np.mean(f1_scores_df.f1_macro):.4f}\")\n",
    "        print(classification_report(Y_test, Y_pred, target_names=label_names))\n",
    "\n",
    "\n",
    "    # Combine all models into one DataFrame\n",
    "    final_results_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    final_results_df.to_csv('../out/model_performance_summary_tf_idf.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for best parameters for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "✅ Best parameters (global) for base LR: {'estimator__C': np.float64(483.2930238571752), 'estimator__penalty': 'l2', 'estimator__solver': 'liblinear'}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "✅ Best parameters (global) for balanced LR: {'estimator__C': np.float64(12.742749857031322), 'estimator__penalty': 'l2', 'estimator__solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Logistic Regression Hyperparameter Grid\n",
    "param_grid_lr = {\n",
    "    'estimator__C': np.logspace(-3, 3, 20),\n",
    "    'estimator__penalty': ['l1', 'l2'],\n",
    "    'estimator__solver': ['liblinear']  # Only solver supporting l1\n",
    "}\n",
    "\n",
    "# Model wrapper\n",
    "base_lr = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "balanced_lr = OneVsRestClassifier(LogisticRegression(class_weight='balanced', max_iter=1000))\n",
    "\n",
    "# For storing metrics\n",
    "# all_f1_scores = []\n",
    "\n",
    "# # Grid Search for base LR\n",
    "# for fold_idx, (train_idx, test_idx) in enumerate(stratifier.split(X, Y)):\n",
    "#     print(f\"\\n📂 Fold {fold_idx + 1}/{n_splits}\")\n",
    "\n",
    "#     X_train, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "#     # Grid search\n",
    "#     grid_search = GridSearchCV(base_lr, param_grid_lr, cv=3, scoring='f1_micro', verbose=1, n_jobs=-1)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     print(f\"Best params for Base LR: {grid_search.best_params_}\")\n",
    "\n",
    "#     y_pred = best_model.predict(X_test)\n",
    "#     y_pred = y_pred.toarray() if hasattr(y_pred, \"toarray\") else y_pred\n",
    "\n",
    "#     f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "#     all_f1_scores.append(f1)\n",
    "\n",
    "#     print(f\"F1 Micro: {f1:.4f}\")\n",
    "#     print(classification_report(y_test, y_pred, target_names=label_names))\n",
    "\n",
    "# print(f\"\\n✅ Average F1 Micro over all folds: {np.mean(all_f1_scores):.4f}\")\n",
    "\n",
    "# # Grid Search for base LR\n",
    "# for fold_idx, (train_idx, test_idx) in enumerate(stratifier.split(X, Y)):\n",
    "#     print(f\"\\n📂 Fold {fold_idx + 1}/{n_splits}\")\n",
    "\n",
    "#     X_train, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "#     # Grid search\n",
    "#     grid_search = GridSearchCV(balanced_lr, param_grid_lr, cv=3, scoring='f1_micro', verbose=1, n_jobs=-1)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     print(f\"Best params for Balanced LR: {grid_search.best_params_}\")\n",
    "\n",
    "#     y_pred = best_model.predict(X_test)\n",
    "#     y_pred = y_pred.toarray() if hasattr(y_pred, \"toarray\") else y_pred\n",
    "\n",
    "#     f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "#     all_f1_scores.append(f1)\n",
    "\n",
    "#     print(f\"F1 Micro: {f1:.4f}\")\n",
    "#     print(classification_report(y_test, y_pred, target_names=label_names))\n",
    "\n",
    "# print(f\"\\n✅ Average F1 Micro over all folds: {np.mean(all_f1_scores):.4f}\")\n",
    "\n",
    "grid_search_base_lr = GridSearchCV(\n",
    "    OneVsRestClassifier(LogisticRegression(max_iter=1000)),\n",
    "    param_grid_lr,\n",
    "    cv=stratifier,\n",
    "    scoring='f1_micro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_base_lr.fit(X, Y)\n",
    "print(\"✅ Best parameters (global) for base LR:\", grid_search_base_lr.best_params_)\n",
    "\n",
    "grid_search_balanced_lr = GridSearchCV(\n",
    "    OneVsRestClassifier(LogisticRegression(class_weight='balanced', max_iter=1000)),\n",
    "    param_grid_lr,\n",
    "    cv=stratifier,\n",
    "    scoring='f1_micro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_balanced_lr.fit(X, Y)\n",
    "print(\"✅ Best parameters (global) for balanced LR:\", grid_search_balanced_lr.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for Random Forest Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 126 candidates, totalling 1260 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Random Forest Hyperparameter Grid\n",
    "param_grid_rf = {\n",
    "    'estimator__n_estimators': [50, 75, 100, 125, 150, 175, 200],\n",
    "    'estimator__max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'estimator__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Model wrapper\n",
    "base_rf = OneVsRestClassifier(RandomForestClassifier())\n",
    "\n",
    "# # For storing metrics\n",
    "# all_f1_scores = []\n",
    "\n",
    "# # Grid Search for base RF\n",
    "# for fold_idx, (train_idx, test_idx) in enumerate(stratifier.split(X, Y)):\n",
    "#     print(f\"\\n📂 Fold {fold_idx + 1}/{n_splits}\")\n",
    "\n",
    "#     X_train, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "#     # Grid search\n",
    "#     grid_search = GridSearchCV(base_rf, param_grid_rf, cv=3, scoring='f1_micro', verbose=1, n_jobs=-1)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     print(f\"Best params for Random Forest: {grid_search.best_params_}\")\n",
    "\n",
    "#     y_pred = best_model.predict(X_test)\n",
    "#     y_pred = y_pred.toarray() if hasattr(y_pred, \"toarray\") else y_pred\n",
    "\n",
    "#     f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "#     all_f1_scores.append(f1)\n",
    "\n",
    "#     print(f\"F1 Micro: {f1:.4f}\")\n",
    "#     print(classification_report(y_test, y_pred, target_names=label_names))\n",
    "\n",
    "# print(f\"\\n✅ Average F1 Micro over all folds: {np.mean(all_f1_scores):.4f}\")\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    OneVsRestClassifier(RandomForestClassifier()),\n",
    "    param_grid_rf,\n",
    "    cv=stratifier,\n",
    "    scoring='f1_micro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_rf.fit(X, Y)\n",
    "print(\"✅ Best parameters (global) for Random Forest:\", grid_search_rf.best_params_)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
