{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced BoW model (TF-IDF vectorization)\n",
    "\n",
    "TF-IDF model using a distinctive terms vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\felix\\Documents\\GIT\\Hertie\\PACT-ML\\modules\n",
      "Added to path: c:\\Users\\felix\\Documents\\GIT\\Hertie\\PACT-ML\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Add the parent directory to the path if needed\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "print(f\"Added to path: {parent_dir}\")\n",
    "\n",
    "# set global seeds for reproducability\n",
    "random.seed(161)\n",
    "np.random.seed(161)\n",
    "os.environ['PYTHONHASHSEED'] = '161'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyreadr import read_r\n",
    "from modules.helpers.validity_check import fuzzy_match_report_key\n",
    "\n",
    "para_data = pd.read_csv(\"..\\data\\PACT_paragraphs_training.csv\")\n",
    "\n",
    "report_data = pd.read_csv(\"..\\data\\paragraphs.csv\", sep=';')\n",
    "\n",
    "report_data[\"matchingKey\"] = report_data[\"report_namePKO\"].str.replace('/', '_')\n",
    "\n",
    "# reduced data to 7 target categories with most codings\n",
    "target_categories = [\n",
    "    \"PoliceReform\",\n",
    "    \"Operations_PatrolsInterventions\",\n",
    "    \"StateAdministration\",\n",
    "    \"RefugeeAssistance\",\n",
    "    \"ElectionAssistance\",\n",
    "    \"LegalReform\",\n",
    "    \"CivilSocietyAssistance\"\n",
    "]\n",
    "\n",
    "report_data = report_data[[\"matchingKey\", \"paragraphNumber\"] + target_categories]\n",
    "\n",
    "report_data[target_categories] = report_data[target_categories].map(lambda x: isinstance(x, str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of merged data: (6029, 12)\n",
      "Number of features for TF-IDF: 6144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "merged_data = pd.merge(\n",
    "    para_data,\n",
    "    report_data,\n",
    "    on=[\"matchingKey\", \"paragraphNumber\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "merged_data.to_csv(\"../data/merged_data.csv\", index=False)\n",
    "\n",
    "# Check the shape of the merged data\n",
    "print(f\"Shape of merged data: {merged_data.shape}\")\n",
    "\n",
    "# vectorizer = CountVectorizer(stop_words='english', min_df=5, max_df=0.95)\n",
    "# X = vectorizer.fit_transform(merged_data[\"paragraph\"])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.95)\n",
    "X = tfidf_vectorizer.fit_transform(merged_data['paragraph'])\n",
    "\n",
    "# df_bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_tfidf = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "# print(f\"Number of features for Count Vectorizer: {df_bow.shape[1]}\")\n",
    "print(f\"Number of features for TF-IDF: {df_tfidf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Stratification K-fold CV\n",
    "\n",
    "To make the most of our limited multi-label data, we use Iterative Stratification K-fold CV across all our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-7>:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "\n",
    "Y = merged_data[target_categories].fillna(False).astype(int).values\n",
    "\n",
    "# Set up Iterative Stratification\n",
    "n_splits = 5\n",
    "stratifier = IterativeStratification(n_splits=n_splits, order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Model + Random Forest using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Logistic Regression ---\n",
      "Average F1 (micro): 0.5145\n",
      "Average F1 (macro): 0.4762\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.62      0.57      0.59       105\n",
      "Operations_PatrolsInterventions       0.67      0.47      0.55        30\n",
      "            StateAdministration       0.62      0.34      0.44        44\n",
      "              RefugeeAssistance       0.60      0.30      0.40        20\n",
      "             ElectionAssistance       0.83      0.43      0.57        23\n",
      "                    LegalReform       0.47      0.27      0.34        26\n",
      "         CivilSocietyAssistance       0.55      0.36      0.44        33\n",
      "\n",
      "                      micro avg       0.62      0.44      0.51       281\n",
      "                      macro avg       0.62      0.39      0.48       281\n",
      "                   weighted avg       0.62      0.44      0.51       281\n",
      "                    samples avg       0.09      0.08      0.08       281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.5178\n",
      "Average F1 (macro): 0.4805\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.65      0.51      0.57       105\n",
      "Operations_PatrolsInterventions       0.67      0.52      0.58        31\n",
      "            StateAdministration       0.68      0.43      0.53        44\n",
      "              RefugeeAssistance       0.71      0.24      0.36        21\n",
      "             ElectionAssistance       0.64      0.41      0.50        22\n",
      "                    LegalReform       0.70      0.28      0.40        25\n",
      "         CivilSocietyAssistance       0.47      0.44      0.45        32\n",
      "\n",
      "                      micro avg       0.63      0.44      0.52       280\n",
      "                      macro avg       0.65      0.40      0.48       280\n",
      "                   weighted avg       0.64      0.44      0.52       280\n",
      "                    samples avg       0.08      0.08      0.08       280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.5104\n",
      "Average F1 (macro): 0.4689\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.70      0.50      0.59       105\n",
      "Operations_PatrolsInterventions       0.82      0.60      0.69        30\n",
      "            StateAdministration       0.58      0.35      0.43        43\n",
      "              RefugeeAssistance       0.60      0.30      0.40        20\n",
      "             ElectionAssistance       0.60      0.26      0.36        23\n",
      "                    LegalReform       0.41      0.27      0.33        26\n",
      "         CivilSocietyAssistance       0.58      0.22      0.32        32\n",
      "\n",
      "                      micro avg       0.65      0.40      0.50       279\n",
      "                      macro avg       0.61      0.36      0.45       279\n",
      "                   weighted avg       0.64      0.40      0.49       279\n",
      "                    samples avg       0.08      0.07      0.08       279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.4965\n",
      "Average F1 (macro): 0.4626\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.54      0.44      0.48       105\n",
      "Operations_PatrolsInterventions       0.75      0.40      0.52        30\n",
      "            StateAdministration       0.55      0.27      0.36        44\n",
      "              RefugeeAssistance       0.67      0.29      0.40        21\n",
      "             ElectionAssistance       0.53      0.41      0.46        22\n",
      "                    LegalReform       0.62      0.31      0.41        26\n",
      "         CivilSocietyAssistance       0.54      0.41      0.46        32\n",
      "\n",
      "                      micro avg       0.57      0.38      0.45       280\n",
      "                      macro avg       0.60      0.36      0.44       280\n",
      "                   weighted avg       0.58      0.38      0.45       280\n",
      "                    samples avg       0.08      0.07      0.07       280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.4994\n",
      "Average F1 (macro): 0.4658\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.68      0.51      0.59       105\n",
      "Operations_PatrolsInterventions       0.80      0.53      0.64        30\n",
      "            StateAdministration       0.67      0.32      0.43        44\n",
      "              RefugeeAssistance       0.41      0.33      0.37        21\n",
      "             ElectionAssistance       0.69      0.41      0.51        22\n",
      "                    LegalReform       0.62      0.31      0.41        26\n",
      "         CivilSocietyAssistance       0.48      0.34      0.40        32\n",
      "\n",
      "                      micro avg       0.64      0.42      0.51       280\n",
      "                      macro avg       0.62      0.39      0.48       280\n",
      "                   weighted avg       0.64      0.42      0.51       280\n",
      "                    samples avg       0.09      0.08      0.08       280\n",
      "\n",
      "\n",
      "--- Balanced Logistic Regression ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.5475\n",
      "Average F1 (macro): 0.5224\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.48      0.84      0.61       105\n",
      "Operations_PatrolsInterventions       0.47      0.80      0.59        30\n",
      "            StateAdministration       0.46      0.70      0.55        44\n",
      "              RefugeeAssistance       0.37      0.70      0.48        20\n",
      "             ElectionAssistance       0.35      0.77      0.49        22\n",
      "                    LegalReform       0.31      0.68      0.42        25\n",
      "         CivilSocietyAssistance       0.38      0.76      0.51        33\n",
      "\n",
      "                      micro avg       0.42      0.77      0.55       279\n",
      "                      macro avg       0.40      0.75      0.52       279\n",
      "                   weighted avg       0.43      0.77      0.55       279\n",
      "                    samples avg       0.13      0.14      0.13       279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.5312\n",
      "Average F1 (macro): 0.5052\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.48      0.76      0.59       105\n",
      "Operations_PatrolsInterventions       0.31      0.63      0.41        30\n",
      "            StateAdministration       0.44      0.66      0.53        44\n",
      "              RefugeeAssistance       0.32      0.52      0.40        21\n",
      "             ElectionAssistance       0.34      0.78      0.47        23\n",
      "                    LegalReform       0.44      0.81      0.57        26\n",
      "         CivilSocietyAssistance       0.34      0.66      0.45        32\n",
      "\n",
      "                      micro avg       0.40      0.71      0.51       281\n",
      "                      macro avg       0.38      0.69      0.49       281\n",
      "                   weighted avg       0.41      0.71      0.52       281\n",
      "                    samples avg       0.12      0.13      0.12       281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.5338\n",
      "Average F1 (macro): 0.5091\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.48      0.79      0.60       105\n",
      "Operations_PatrolsInterventions       0.47      0.87      0.61        30\n",
      "            StateAdministration       0.42      0.73      0.53        44\n",
      "              RefugeeAssistance       0.39      0.76      0.52        21\n",
      "             ElectionAssistance       0.38      0.68      0.49        22\n",
      "                    LegalReform       0.35      0.58      0.43        26\n",
      "         CivilSocietyAssistance       0.32      0.66      0.43        32\n",
      "\n",
      "                      micro avg       0.42      0.74      0.54       280\n",
      "                      macro avg       0.40      0.72      0.52       280\n",
      "                   weighted avg       0.43      0.74      0.54       280\n",
      "                    samples avg       0.13      0.14      0.13       280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.5270\n",
      "Average F1 (macro): 0.5045\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.45      0.70      0.55       105\n",
      "Operations_PatrolsInterventions       0.43      0.67      0.53        30\n",
      "            StateAdministration       0.42      0.60      0.50        43\n",
      "              RefugeeAssistance       0.34      0.57      0.43        21\n",
      "             ElectionAssistance       0.39      0.73      0.51        22\n",
      "                    LegalReform       0.31      0.69      0.43        26\n",
      "         CivilSocietyAssistance       0.36      0.81      0.50        32\n",
      "\n",
      "                      micro avg       0.40      0.69      0.51       279\n",
      "                      macro avg       0.39      0.68      0.49       279\n",
      "                   weighted avg       0.41      0.69      0.51       279\n",
      "                    samples avg       0.12      0.13      0.12       279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.5280\n",
      "Average F1 (macro): 0.5060\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.51      0.76      0.61       105\n",
      "Operations_PatrolsInterventions       0.46      0.84      0.59        31\n",
      "            StateAdministration       0.33      0.57      0.42        44\n",
      "              RefugeeAssistance       0.32      0.60      0.42        20\n",
      "             ElectionAssistance       0.43      0.87      0.57        23\n",
      "                    LegalReform       0.32      0.77      0.45        26\n",
      "         CivilSocietyAssistance       0.40      0.72      0.52        32\n",
      "\n",
      "                      micro avg       0.42      0.73      0.53       281\n",
      "                      macro avg       0.40      0.73      0.51       281\n",
      "                   weighted avg       0.43      0.73      0.54       281\n",
      "                    samples avg       0.12      0.13      0.12       281\n",
      "\n",
      "\n",
      "--- Random Forest ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.1438\n",
      "Average F1 (macro): 0.1038\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.92      0.10      0.19       105\n",
      "Operations_PatrolsInterventions       0.88      0.23      0.37        30\n",
      "            StateAdministration       1.00      0.09      0.17        43\n",
      "              RefugeeAssistance       0.00      0.00      0.00        21\n",
      "             ElectionAssistance       0.00      0.00      0.00        22\n",
      "                    LegalReform       0.00      0.00      0.00        26\n",
      "         CivilSocietyAssistance       0.00      0.00      0.00        32\n",
      "\n",
      "                      micro avg       0.81      0.08      0.14       279\n",
      "                      macro avg       0.40      0.06      0.10       279\n",
      "                   weighted avg       0.59      0.08      0.14       279\n",
      "                    samples avg       0.01      0.01      0.01       279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.1823\n",
      "Average F1 (macro): 0.1448\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       1.00      0.17      0.29       105\n",
      "Operations_PatrolsInterventions       1.00      0.20      0.33        30\n",
      "            StateAdministration       0.83      0.11      0.20        44\n",
      "              RefugeeAssistance       1.00      0.10      0.18        20\n",
      "             ElectionAssistance       1.00      0.09      0.16        23\n",
      "                    LegalReform       1.00      0.04      0.07        26\n",
      "         CivilSocietyAssistance       1.00      0.03      0.06        33\n",
      "\n",
      "                      micro avg       0.97      0.12      0.22       281\n",
      "                      macro avg       0.98      0.11      0.19       281\n",
      "                   weighted avg       0.97      0.12      0.22       281\n",
      "                    samples avg       0.02      0.02      0.02       281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.1778\n",
      "Average F1 (macro): 0.1341\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.94      0.16      0.28       105\n",
      "Operations_PatrolsInterventions       1.00      0.13      0.24        30\n",
      "            StateAdministration       1.00      0.07      0.13        44\n",
      "              RefugeeAssistance       1.00      0.05      0.09        21\n",
      "             ElectionAssistance       0.00      0.00      0.00        22\n",
      "                    LegalReform       0.00      0.00      0.00        26\n",
      "         CivilSocietyAssistance       0.50      0.03      0.06        32\n",
      "\n",
      "                      micro avg       0.93      0.09      0.17       280\n",
      "                      macro avg       0.63      0.06      0.11       280\n",
      "                   weighted avg       0.75      0.09      0.16       280\n",
      "                    samples avg       0.02      0.02      0.02       280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.1781\n",
      "Average F1 (macro): 0.1361\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.86      0.11      0.20       105\n",
      "Operations_PatrolsInterventions       1.00      0.10      0.18        31\n",
      "            StateAdministration       1.00      0.23      0.37        44\n",
      "              RefugeeAssistance       1.00      0.05      0.09        21\n",
      "             ElectionAssistance       1.00      0.04      0.08        23\n",
      "                    LegalReform       0.50      0.04      0.07        26\n",
      "         CivilSocietyAssistance       0.00      0.00      0.00        32\n",
      "\n",
      "                      micro avg       0.90      0.10      0.18       282\n",
      "                      macro avg       0.77      0.08      0.14       282\n",
      "                   weighted avg       0.79      0.10      0.17       282\n",
      "                    samples avg       0.02      0.02      0.02       282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.1739\n",
      "Average F1 (macro): 0.1313\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.81      0.12      0.21       105\n",
      "Operations_PatrolsInterventions       1.00      0.13      0.24        30\n",
      "            StateAdministration       1.00      0.14      0.24        44\n",
      "              RefugeeAssistance       1.00      0.05      0.10        20\n",
      "             ElectionAssistance       0.00      0.00      0.00        22\n",
      "                    LegalReform       0.00      0.00      0.00        25\n",
      "         CivilSocietyAssistance       0.00      0.00      0.00        32\n",
      "\n",
      "                      micro avg       0.89      0.09      0.16       278\n",
      "                      macro avg       0.54      0.06      0.11       278\n",
      "                   weighted avg       0.65      0.09      0.15       278\n",
      "                    samples avg       0.02      0.02      0.02       278\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, multilabel_confusion_matrix, precision_score, recall_score\n",
    "\n",
    "best_params_rf = {'estimator__max_depth': None, 'estimator__min_samples_split': 2, 'estimator__n_estimators': 75}\n",
    "\n",
    "rf_params = {k.replace('estimator__', ''): v for k, v in best_params_rf.items()}\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": OneVsRestClassifier(LogisticRegression(max_iter=1000, C= 10.0, penalty='l1', solver='liblinear')),\n",
    "    \"Balanced Logistic Regression\": OneVsRestClassifier(LogisticRegression(class_weight='balanced', max_iter=1000, C= 1.0, penalty='l2', solver='liblinear')),\n",
    "    \"Random Forest\": RandomForestClassifier(**rf_params)\n",
    "}\n",
    "\n",
    "label_names = merged_data[target_categories].columns.tolist()\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "\n",
    "    # Store metrics per fold, per label\n",
    "    fold_metrics = []\n",
    "\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_idx, test_idx in stratifier.split(X, Y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "\n",
    "        Y_pred = Y_pred.toarray() if hasattr(Y_pred, 'toarray') else Y_pred\n",
    "\n",
    "        f1_micro = f1_score(Y_test, Y_pred, average='micro')  # or 'macro'\n",
    "        f1_macro = f1_score(Y_test, Y_pred, average='macro')\n",
    "        # save to display later\n",
    "        f1_scores.append({'f1_micro': f1_micro, 'f1_macro': f1_macro})\n",
    "        f1_scores_df = pd.DataFrame(f1_scores)\n",
    "\n",
    "        # Compute confusion matrices per label\n",
    "        cm = multilabel_confusion_matrix(Y_test, Y_pred)\n",
    "        precision_per_label = precision_score(Y_test, Y_pred, average=None, zero_division=0)\n",
    "        recall_per_label = recall_score(Y_test, Y_pred, average=None, zero_division=0)\n",
    "\n",
    "        # Show false posotives and false negatives per label\n",
    "        for i, label in enumerate(label_names):\n",
    "            tn, fp, fn, tp = cm[i].ravel()\n",
    "\n",
    "            # Use sklearn's calculated metrics\n",
    "            precision = precision_per_label[i]\n",
    "            recall = recall_per_label[i]\n",
    "\n",
    "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "            fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "            tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "            fold_metrics.append({\n",
    "                'Model': model_name,\n",
    "                'Fold': len(fold_metrics) // len(label_names) + 1,\n",
    "                'Label': label,\n",
    "                'F1_micro': f1_micro,\n",
    "                'F1_macro': f1_macro,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'FPR': fpr,\n",
    "                'FNR': fnr,\n",
    "                'TPR': tpr,\n",
    "                'TNR': tnr,\n",
    "                'TP': tp,\n",
    "                'FP': fp,\n",
    "                'FN': fn,\n",
    "                'TN': tn\n",
    "            })\n",
    "\n",
    "        # Convert per-fold metrics to DataFrame\n",
    "        fold_metrics_df = pd.DataFrame(fold_metrics)\n",
    "\n",
    "        # Append summary to global results list\n",
    "        results.append(fold_metrics_df)\n",
    "\n",
    "        print(f\"Average F1 (micro): {np.mean(f1_scores_df.f1_micro):.4f}\")\n",
    "        print(f\"Average F1 (macro): {np.mean(f1_scores_df.f1_macro):.4f}\")\n",
    "        print(classification_report(Y_test, Y_pred, target_names=label_names))\n",
    "\n",
    "\n",
    "    # Combine all models into one DataFrame\n",
    "    final_results_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    final_results_df.to_csv('../out/model_performance_summary_tf_idf.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for best parameters for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "✅ Best parameters (global) for base LR: {'estimator__C': np.float64(483.2930238571752), 'estimator__penalty': 'l2', 'estimator__solver': 'liblinear'}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "✅ Best parameters (global) for balanced LR: {'estimator__C': np.float64(12.742749857031322), 'estimator__penalty': 'l2', 'estimator__solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Logistic Regression Hyperparameter Grid\n",
    "param_grid_lr = {\n",
    "    'estimator__C': np.logspace(-3, 3, 20),\n",
    "    'estimator__penalty': ['l1', 'l2'],\n",
    "    'estimator__solver': ['liblinear']  # Only solver supporting l1\n",
    "}\n",
    "\n",
    "# Model wrapper\n",
    "base_lr = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "balanced_lr = OneVsRestClassifier(LogisticRegression(class_weight='balanced', max_iter=1000))\n",
    "\n",
    "# For storing metrics\n",
    "# all_f1_scores = []\n",
    "\n",
    "# # Grid Search for base LR\n",
    "# for fold_idx, (train_idx, test_idx) in enumerate(stratifier.split(X, Y)):\n",
    "#     print(f\"\\n📂 Fold {fold_idx + 1}/{n_splits}\")\n",
    "\n",
    "#     X_train, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "#     # Grid search\n",
    "#     grid_search = GridSearchCV(base_lr, param_grid_lr, cv=3, scoring='f1_micro', verbose=1, n_jobs=-1)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     print(f\"Best params for Base LR: {grid_search.best_params_}\")\n",
    "\n",
    "#     y_pred = best_model.predict(X_test)\n",
    "#     y_pred = y_pred.toarray() if hasattr(y_pred, \"toarray\") else y_pred\n",
    "\n",
    "#     f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "#     all_f1_scores.append(f1)\n",
    "\n",
    "#     print(f\"F1 Micro: {f1:.4f}\")\n",
    "#     print(classification_report(y_test, y_pred, target_names=label_names))\n",
    "\n",
    "# print(f\"\\n✅ Average F1 Micro over all folds: {np.mean(all_f1_scores):.4f}\")\n",
    "\n",
    "# # Grid Search for base LR\n",
    "# for fold_idx, (train_idx, test_idx) in enumerate(stratifier.split(X, Y)):\n",
    "#     print(f\"\\n📂 Fold {fold_idx + 1}/{n_splits}\")\n",
    "\n",
    "#     X_train, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "#     # Grid search\n",
    "#     grid_search = GridSearchCV(balanced_lr, param_grid_lr, cv=3, scoring='f1_micro', verbose=1, n_jobs=-1)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     print(f\"Best params for Balanced LR: {grid_search.best_params_}\")\n",
    "\n",
    "#     y_pred = best_model.predict(X_test)\n",
    "#     y_pred = y_pred.toarray() if hasattr(y_pred, \"toarray\") else y_pred\n",
    "\n",
    "#     f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "#     all_f1_scores.append(f1)\n",
    "\n",
    "#     print(f\"F1 Micro: {f1:.4f}\")\n",
    "#     print(classification_report(y_test, y_pred, target_names=label_names))\n",
    "\n",
    "# print(f\"\\n✅ Average F1 Micro over all folds: {np.mean(all_f1_scores):.4f}\")\n",
    "\n",
    "grid_search_base_lr = GridSearchCV(\n",
    "    OneVsRestClassifier(LogisticRegression(max_iter=1000)),\n",
    "    param_grid_lr,\n",
    "    cv=stratifier,\n",
    "    scoring='f1_micro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_base_lr.fit(X, Y)\n",
    "print(\"✅ Best parameters (global) for base LR:\", grid_search_base_lr.best_params_)\n",
    "\n",
    "grid_search_balanced_lr = GridSearchCV(\n",
    "    OneVsRestClassifier(LogisticRegression(class_weight='balanced', max_iter=1000)),\n",
    "    param_grid_lr,\n",
    "    cv=stratifier,\n",
    "    scoring='f1_micro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_balanced_lr.fit(X, Y)\n",
    "print(\"✅ Best parameters (global) for balanced LR:\", grid_search_balanced_lr.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for Random Forest Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 126 candidates, totalling 1260 fits\n",
      "✅ Best parameters (global) for Random Forest: {'estimator__max_depth': 40, 'estimator__min_samples_split': 2, 'estimator__n_estimators': 75}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest Hyperparameter Grid\n",
    "param_grid_rf = {\n",
    "    'estimator__n_estimators': [50, 75, 100, 125, 150, 175, 200],\n",
    "    'estimator__max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'estimator__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Model wrapper\n",
    "base_rf = OneVsRestClassifier(RandomForestClassifier())\n",
    "\n",
    "# # For storing metrics\n",
    "# all_f1_scores = []\n",
    "\n",
    "# # Grid Search for base RF\n",
    "# for fold_idx, (train_idx, test_idx) in enumerate(stratifier.split(X, Y)):\n",
    "#     print(f\"\\n📂 Fold {fold_idx + 1}/{n_splits}\")\n",
    "\n",
    "#     X_train, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "#     # Grid search\n",
    "#     grid_search = GridSearchCV(base_rf, param_grid_rf, cv=3, scoring='f1_micro', verbose=1, n_jobs=-1)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     print(f\"Best params for Random Forest: {grid_search.best_params_}\")\n",
    "\n",
    "#     y_pred = best_model.predict(X_test)\n",
    "#     y_pred = y_pred.toarray() if hasattr(y_pred, \"toarray\") else y_pred\n",
    "\n",
    "#     f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "#     all_f1_scores.append(f1)\n",
    "\n",
    "#     print(f\"F1 Micro: {f1:.4f}\")\n",
    "#     print(classification_report(y_test, y_pred, target_names=label_names))\n",
    "\n",
    "# print(f\"\\n✅ Average F1 Micro over all folds: {np.mean(all_f1_scores):.4f}\")\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    OneVsRestClassifier(RandomForestClassifier()),\n",
    "    param_grid_rf,\n",
    "    cv=stratifier,\n",
    "    scoring='f1_micro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_rf.fit(X, Y)\n",
    "print(\"✅ Best parameters (global) for Random Forest:\", grid_search_rf.best_params_)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
