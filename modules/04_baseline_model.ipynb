{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model (Bag of Words approach)\n",
    "\n",
    "Naive model using a non-contextual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\felix\\Documents\\GIT\\Hertie\\PACT-ML\\modules\n",
      "Added to path: c:\\Users\\felix\\Documents\\GIT\\Hertie\\PACT-ML\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Add the parent directory to the path if needed\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "print(f\"Added to path: {parent_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyreadr import read_r\n",
    "from modules.helpers.validity_check import fuzzy_match_report_key\n",
    "\n",
    "para_data = pd.read_csv(\"..\\data\\PACT_paragraphs_training.csv\")\n",
    "\n",
    "report_data = pd.read_csv(\"..\\data\\paragraphs.csv\", sep=';')\n",
    "\n",
    "report_data[\"matchingKey\"] = report_data[\"report_namePKO\"].str.replace('/', '_')\n",
    "\n",
    "# reduced data to 7 target categories with most codings\n",
    "target_categories = [\n",
    "    \"PoliceReform\",\n",
    "    \"Operations_PatrolsInterventions\",\n",
    "    \"StateAdministration\",\n",
    "    \"RefugeeAssistance\",\n",
    "    \"ElectionAssistance\",\n",
    "    \"LegalReform\",\n",
    "    \"CivilSocietyAssistance\"\n",
    "]\n",
    "\n",
    "report_data = report_data[[\"matchingKey\", \"paragraphNumber\"] + target_categories]\n",
    "\n",
    "report_data[target_categories] = report_data[target_categories].map(lambda x: isinstance(x, str))\n",
    "\n",
    "# check if a paragraph led to two codings\n",
    "multi_coded[\"num_labels\"] = report_data[target_categories].sum(axis=1)\n",
    "\n",
    "multi_coded = multi_coded[report_data[\"num_labels\"] > 1]\n",
    "\n",
    "print(f\"{len(multi_coded)} paragraphs have multiple codings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows matching on both matchingKey and paragraphNumber: 1819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(29)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# number of coded paragraphs in PACT2.0 that have text in the training data\n",
    "merged = pd.merge(\n",
    "    report_data[[\"matchingKey\", \"paragraphNumber\"]],\n",
    "    para_data[[\"matchingKey\", \"paragraphNumber\"]],\n",
    "    on=[\"matchingKey\", \"paragraphNumber\"],\n",
    "    how=\"left\",\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "# Count matches (both columns match)\n",
    "matching_pairs = (merged[\"_merge\"] == \"both\").sum()\n",
    "print(f\"Rows matching on both matchingKey and paragraphNumber: {matching_pairs}\")\n",
    "\n",
    "# number of paragraphs for which parsing failed\n",
    "report_data[\"matchingKey\"].isin(para_data[\"matchingKey\"]).sum() - matching_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(para_data)\n",
    "\n",
    "df_bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data needs to be appended to the parsed paragraph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Model using BoW\n",
    "Now, we can finally calculate our first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
