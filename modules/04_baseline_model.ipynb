{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model (Bag of Words approach)\n",
    "\n",
    "Naive model using a non-contextual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\felix\\Documents\\GIT\\Hertie\\PACT-ML\\modules\n",
      "Added to path: c:\\Users\\felix\\Documents\\GIT\\Hertie\\PACT-ML\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Add the parent directory to the path if needed\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "print(f\"Added to path: {parent_dir}\")\n",
    "\n",
    "# set global seeds for reproducability\n",
    "random.seed(161)\n",
    "np.random.seed(161)\n",
    "os.environ['PYTHONHASHSEED'] = '161'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472 paragraphs have multiple codings.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyreadr import read_r\n",
    "from modules.helpers.validity_check import fuzzy_match_report_key\n",
    "\n",
    "para_data = pd.read_csv(\"..\\data\\PACT_paragraphs_training.csv\")\n",
    "\n",
    "report_data = pd.read_csv(\"..\\data\\paragraphs.csv\", sep=';')\n",
    "\n",
    "report_data[\"matchingKey\"] = report_data[\"report_namePKO\"].str.replace('/', '_')\n",
    "\n",
    "# reduced data to 7 target categories with most codings\n",
    "target_categories = [\n",
    "    \"PoliceReform\",\n",
    "    \"Operations_PatrolsInterventions\",\n",
    "    \"StateAdministration\",\n",
    "    \"RefugeeAssistance\",\n",
    "    \"ElectionAssistance\",\n",
    "    \"LegalReform\",\n",
    "    \"CivilSocietyAssistance\"\n",
    "]\n",
    "\n",
    "report_data = report_data[[\"matchingKey\", \"paragraphNumber\"] + target_categories]\n",
    "\n",
    "report_data[target_categories] = report_data[target_categories].map(lambda x: isinstance(x, str))\n",
    "\n",
    "# check if a paragraph led to two codings\n",
    "multi_coded = report_data.copy()\n",
    "multi_coded[\"num_labels\"] = report_data[target_categories].sum(axis=1)\n",
    "\n",
    "multi_coded = multi_coded[multi_coded[\"num_labels\"] > 1]\n",
    "\n",
    "print(f\"{len(multi_coded)} paragraphs have multiple codings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows matching on both matchingKey and paragraphNumber: 1819\n",
      "Rows matching on both matchingKey and paragraphNumber for multi-coded paragraphs: 256\n",
      "Number of coded paragraphs for which text parsing failed: 29\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# number of coded paragraphs in PACT2.0 that have text in the training data\n",
    "merged = pd.merge(\n",
    "    report_data[[\"matchingKey\", \"paragraphNumber\"]],\n",
    "    para_data[[\"matchingKey\", \"paragraphNumber\"]],\n",
    "    on=[\"matchingKey\", \"paragraphNumber\"],\n",
    "    how=\"left\",\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "matching_pairs = (merged[\"_merge\"] == \"both\").sum()\n",
    "print(f\"Rows matching on both matchingKey and paragraphNumber: {matching_pairs}\")\n",
    "\n",
    "# number of multi-label paragraphs in PACT2.0 that have text in the training data\n",
    "merged_multi = pd.merge(\n",
    "    multi_coded[[\"matchingKey\", \"paragraphNumber\"]],\n",
    "    para_data[[\"matchingKey\", \"paragraphNumber\"]],\n",
    "    on=[\"matchingKey\", \"paragraphNumber\"],\n",
    "    how=\"left\",\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "matching_pairs_multi = (merged_multi[\"_merge\"] == \"both\").sum()\n",
    "print(f\"Rows matching on both matchingKey and paragraphNumber for multi-coded paragraphs: {matching_pairs_multi}\")\n",
    "\n",
    "# number of paragraphs for which parsing failed\n",
    "failed_parsing = report_data[\"matchingKey\"].isin(para_data[\"matchingKey\"]).sum() - matching_pairs\n",
    "print(f\"Number of coded paragraphs for which text parsing failed: {failed_parsing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of merged data: (6029, 12)\n",
      "Number of features for Count Vectorizer: 6144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "merged_data = pd.merge(\n",
    "    para_data,\n",
    "    report_data,\n",
    "    on=[\"matchingKey\", \"paragraphNumber\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "merged_data.to_csv(\"../data/merged_data.csv\", index=False)\n",
    "\n",
    "# Check the shape of the merged data\n",
    "print(f\"Shape of merged data: {merged_data.shape}\")\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=5, max_df=0.95)\n",
    "X = vectorizer.fit_transform(merged_data[\"paragraph\"])\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.95)\n",
    "# X_tfidf = tfidf_vectorizer.fit_transform(merged_data['paragraph'])\n",
    "\n",
    "df_bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "# df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(f\"Number of features for Count Vectorizer: {df_bow.shape[1]}\")\n",
    "# print(f\"Number of features for TF-IDF: {df_tfidf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Stratification K-fold CV\n",
    "\n",
    "To make the most of our limited multi-label data, we use Iterative Stratification K-fold CV across all our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-7>:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "\n",
    "Y = merged_data[target_categories].fillna(False).astype(int).values\n",
    "\n",
    "# Set up Iterative Stratification\n",
    "n_splits = 5\n",
    "stratifier = IterativeStratification(n_splits=n_splits, order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Model + Random Forest using BoW\n",
    "Now, we can finally calculate our first two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Logistic Regression ---\n",
      "Average F1 (micro): 0.5217\n",
      "Average F1 (macro): 0.4882\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.68      0.51      0.59       105\n",
      "Operations_PatrolsInterventions       0.62      0.53      0.57        30\n",
      "            StateAdministration       0.57      0.47      0.51        43\n",
      "              RefugeeAssistance       0.53      0.38      0.44        21\n",
      "             ElectionAssistance       0.53      0.36      0.43        22\n",
      "                    LegalReform       0.50      0.38      0.43        26\n",
      "         CivilSocietyAssistance       0.71      0.31      0.43        32\n",
      "\n",
      "                      micro avg       0.62      0.45      0.52       279\n",
      "                      macro avg       0.59      0.42      0.49       279\n",
      "                   weighted avg       0.62      0.45      0.52       279\n",
      "                    samples avg       0.08      0.08      0.08       279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.4982\n",
      "Average F1 (macro): 0.4700\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.56      0.47      0.51       105\n",
      "Operations_PatrolsInterventions       0.75      0.50      0.60        30\n",
      "            StateAdministration       0.61      0.39      0.47        44\n",
      "              RefugeeAssistance       0.67      0.20      0.31        20\n",
      "             ElectionAssistance       0.55      0.52      0.53        23\n",
      "                    LegalReform       0.40      0.31      0.35        26\n",
      "         CivilSocietyAssistance       0.43      0.36      0.39        33\n",
      "\n",
      "                      micro avg       0.55      0.42      0.47       281\n",
      "                      macro avg       0.56      0.39      0.45       281\n",
      "                   weighted avg       0.56      0.42      0.47       281\n",
      "                    samples avg       0.08      0.07      0.07       281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.4912\n",
      "Average F1 (macro): 0.4640\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.58      0.47      0.52       105\n",
      "Operations_PatrolsInterventions       0.69      0.58      0.63        31\n",
      "            StateAdministration       0.56      0.43      0.49        44\n",
      "              RefugeeAssistance       0.36      0.24      0.29        21\n",
      "             ElectionAssistance       0.55      0.50      0.52        22\n",
      "                    LegalReform       0.53      0.31      0.39        26\n",
      "         CivilSocietyAssistance       0.34      0.31      0.33        32\n",
      "\n",
      "                      micro avg       0.54      0.43      0.48       281\n",
      "                      macro avg       0.52      0.41      0.45       281\n",
      "                   weighted avg       0.54      0.43      0.48       281\n",
      "                    samples avg       0.08      0.08      0.08       281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.4939\n",
      "Average F1 (macro): 0.4642\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.63      0.50      0.55       105\n",
      "Operations_PatrolsInterventions       0.68      0.57      0.62        30\n",
      "            StateAdministration       0.61      0.43      0.51        44\n",
      "              RefugeeAssistance       0.44      0.35      0.39        20\n",
      "             ElectionAssistance       0.50      0.26      0.34        23\n",
      "                    LegalReform       0.64      0.28      0.39        25\n",
      "         CivilSocietyAssistance       0.57      0.38      0.45        32\n",
      "\n",
      "                      micro avg       0.60      0.43      0.50       279\n",
      "                      macro avg       0.58      0.39      0.46       279\n",
      "                   weighted avg       0.60      0.43      0.50       279\n",
      "                    samples avg       0.09      0.08      0.08       279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.5019\n",
      "Average F1 (macro): 0.4718\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.69      0.55      0.61       105\n",
      "Operations_PatrolsInterventions       0.68      0.57      0.62        30\n",
      "            StateAdministration       0.57      0.36      0.44        44\n",
      "              RefugeeAssistance       0.75      0.43      0.55        21\n",
      "             ElectionAssistance       0.62      0.36      0.46        22\n",
      "                    LegalReform       0.44      0.31      0.36        26\n",
      "         CivilSocietyAssistance       0.52      0.44      0.47        32\n",
      "\n",
      "                      micro avg       0.63      0.46      0.53       280\n",
      "                      macro avg       0.61      0.43      0.50       280\n",
      "                   weighted avg       0.63      0.46      0.53       280\n",
      "                    samples avg       0.09      0.08      0.08       280\n",
      "\n",
      "\n",
      "--- Balanced Logistic Regression ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.4850\n",
      "Average F1 (macro): 0.4577\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.47      0.84      0.60       105\n",
      "Operations_PatrolsInterventions       0.43      0.61      0.51        31\n",
      "            StateAdministration       0.29      0.75      0.42        44\n",
      "              RefugeeAssistance       0.26      0.70      0.38        20\n",
      "             ElectionAssistance       0.29      0.91      0.43        22\n",
      "                    LegalReform       0.33      0.77      0.47        26\n",
      "         CivilSocietyAssistance       0.27      0.78      0.40        32\n",
      "\n",
      "                      micro avg       0.35      0.78      0.49       280\n",
      "                      macro avg       0.33      0.77      0.46       280\n",
      "                   weighted avg       0.37      0.78      0.50       280\n",
      "                    samples avg       0.12      0.14      0.13       280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.4641\n",
      "Average F1 (macro): 0.4420\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.40      0.68      0.50       105\n",
      "Operations_PatrolsInterventions       0.38      0.77      0.51        30\n",
      "            StateAdministration       0.31      0.73      0.43        44\n",
      "              RefugeeAssistance       0.31      0.71      0.43        21\n",
      "             ElectionAssistance       0.22      0.55      0.32        22\n",
      "                    LegalReform       0.24      0.77      0.37        26\n",
      "         CivilSocietyAssistance       0.29      0.85      0.43        33\n",
      "\n",
      "                      micro avg       0.32      0.72      0.44       281\n",
      "                      macro avg       0.31      0.72      0.43       281\n",
      "                   weighted avg       0.33      0.72      0.45       281\n",
      "                    samples avg       0.12      0.14      0.12       281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.4703\n",
      "Average F1 (macro): 0.4501\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.45      0.77      0.57       105\n",
      "Operations_PatrolsInterventions       0.45      0.83      0.58        30\n",
      "            StateAdministration       0.32      0.66      0.43        44\n",
      "              RefugeeAssistance       0.32      0.85      0.47        20\n",
      "             ElectionAssistance       0.28      0.83      0.41        23\n",
      "                    LegalReform       0.29      0.72      0.41        25\n",
      "         CivilSocietyAssistance       0.26      0.84      0.40        32\n",
      "\n",
      "                      micro avg       0.35      0.77      0.48       279\n",
      "                      macro avg       0.34      0.79      0.47       279\n",
      "                   weighted avg       0.37      0.77      0.49       279\n",
      "                    samples avg       0.13      0.14      0.13       279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.4704\n",
      "Average F1 (macro): 0.4492\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.44      0.80      0.57       105\n",
      "Operations_PatrolsInterventions       0.38      0.80      0.52        30\n",
      "            StateAdministration       0.28      0.77      0.41        43\n",
      "              RefugeeAssistance       0.29      0.71      0.41        21\n",
      "             ElectionAssistance       0.28      0.73      0.40        22\n",
      "                    LegalReform       0.25      0.81      0.38        26\n",
      "         CivilSocietyAssistance       0.31      0.78      0.44        32\n",
      "\n",
      "                      micro avg       0.34      0.78      0.47       279\n",
      "                      macro avg       0.32      0.77      0.45       279\n",
      "                   weighted avg       0.35      0.78      0.48       279\n",
      "                    samples avg       0.12      0.14      0.13       279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.4693\n",
      "Average F1 (macro): 0.4484\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.43      0.79      0.56       105\n",
      "Operations_PatrolsInterventions       0.37      0.80      0.51        30\n",
      "            StateAdministration       0.25      0.61      0.36        44\n",
      "              RefugeeAssistance       0.32      0.90      0.47        21\n",
      "             ElectionAssistance       0.32      0.74      0.45        23\n",
      "                    LegalReform       0.26      0.58      0.36        26\n",
      "         CivilSocietyAssistance       0.29      0.78      0.42        32\n",
      "\n",
      "                      micro avg       0.34      0.75      0.46       281\n",
      "                      macro avg       0.32      0.74      0.45       281\n",
      "                   weighted avg       0.35      0.75      0.47       281\n",
      "                    samples avg       0.12      0.13      0.12       281\n",
      "\n",
      "\n",
      "--- Random Forest ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.1944\n",
      "Average F1 (macro): 0.1360\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.95      0.17      0.29       105\n",
      "Operations_PatrolsInterventions       0.71      0.17      0.27        30\n",
      "            StateAdministration       1.00      0.14      0.24        44\n",
      "              RefugeeAssistance       0.00      0.00      0.00        21\n",
      "             ElectionAssistance       0.50      0.04      0.08        23\n",
      "                    LegalReform       0.50      0.04      0.07        26\n",
      "         CivilSocietyAssistance       0.00      0.00      0.00        32\n",
      "\n",
      "                      micro avg       0.82      0.11      0.19       281\n",
      "                      macro avg       0.52      0.08      0.14       281\n",
      "                   weighted avg       0.67      0.11      0.19       281\n",
      "                    samples avg       0.02      0.02      0.02       281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.2171\n",
      "Average F1 (macro): 0.1568\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.95      0.20      0.33       105\n",
      "Operations_PatrolsInterventions       1.00      0.23      0.38        30\n",
      "            StateAdministration       0.88      0.16      0.27        43\n",
      "              RefugeeAssistance       1.00      0.05      0.10        20\n",
      "             ElectionAssistance       1.00      0.05      0.09        22\n",
      "                    LegalReform       1.00      0.04      0.08        25\n",
      "         CivilSocietyAssistance       0.00      0.00      0.00        32\n",
      "\n",
      "                      micro avg       0.95      0.14      0.24       277\n",
      "                      macro avg       0.83      0.10      0.18       277\n",
      "                   weighted avg       0.85      0.14      0.23       277\n",
      "                    samples avg       0.02      0.02      0.02       277\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.2183\n",
      "Average F1 (macro): 0.1599\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.86      0.18      0.30       105\n",
      "Operations_PatrolsInterventions       1.00      0.20      0.33        30\n",
      "            StateAdministration       1.00      0.16      0.27        44\n",
      "              RefugeeAssistance       1.00      0.05      0.10        20\n",
      "             ElectionAssistance       1.00      0.05      0.09        22\n",
      "                    LegalReform       1.00      0.04      0.07        26\n",
      "         CivilSocietyAssistance       0.00      0.00      0.00        32\n",
      "\n",
      "                      micro avg       0.92      0.13      0.22       279\n",
      "                      macro avg       0.84      0.10      0.17       279\n",
      "                   weighted avg       0.83      0.13      0.21       279\n",
      "                    samples avg       0.02      0.02      0.02       279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.2117\n",
      "Average F1 (macro): 0.1557\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.88      0.14      0.25       105\n",
      "Operations_PatrolsInterventions       1.00      0.17      0.29        30\n",
      "            StateAdministration       1.00      0.18      0.31        44\n",
      "              RefugeeAssistance       1.00      0.05      0.09        21\n",
      "             ElectionAssistance       0.00      0.00      0.00        22\n",
      "                    LegalReform       0.50      0.04      0.07        26\n",
      "         CivilSocietyAssistance       0.00      0.00      0.00        32\n",
      "\n",
      "                      micro avg       0.91      0.11      0.19       280\n",
      "                      macro avg       0.63      0.08      0.14       280\n",
      "                   weighted avg       0.72      0.11      0.18       280\n",
      "                    samples avg       0.02      0.02      0.02       280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.1970\n",
      "Average F1 (macro): 0.1453\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       1.00      0.10      0.19       105\n",
      "Operations_PatrolsInterventions       1.00      0.16      0.28        31\n",
      "            StateAdministration       1.00      0.09      0.17        44\n",
      "              RefugeeAssistance       1.00      0.05      0.09        21\n",
      "             ElectionAssistance       0.00      0.00      0.00        23\n",
      "                    LegalReform       0.00      0.00      0.00        26\n",
      "         CivilSocietyAssistance       0.00      0.00      0.00        33\n",
      "\n",
      "                      micro avg       1.00      0.07      0.14       283\n",
      "                      macro avg       0.57      0.06      0.10       283\n",
      "                   weighted avg       0.71      0.07      0.13       283\n",
      "                    samples avg       0.01      0.01      0.01       283\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, multilabel_confusion_matrix\n",
    "\n",
    "best_params_rf = {'estimator__max_depth': None, 'estimator__min_samples_split': 2, 'estimator__n_estimators': 75}\n",
    "\n",
    "rf_params = {k.replace('estimator__', ''): v for k, v in best_params_rf.items()}\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": OneVsRestClassifier(LogisticRegression(max_iter=1000, C= 1.0, penalty='l1', solver='liblinear')),\n",
    "    \"Balanced Logistic Regression\": OneVsRestClassifier(LogisticRegression(class_weight='balanced', max_iter=1000, C= 0.1, penalty='l1', solver='liblinear')),\n",
    "    \"Random Forest\": RandomForestClassifier(**rf_params)\n",
    "}\n",
    "\n",
    "label_names = merged_data[target_categories].columns.tolist()\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "\n",
    "    # Store metrics per fold, per label\n",
    "    fold_metrics = []\n",
    "\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_idx, test_idx in stratifier.split(X, Y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "\n",
    "        Y_pred = Y_pred.toarray() if hasattr(Y_pred, 'toarray') else Y_pred\n",
    "\n",
    "        f1_micro = f1_score(Y_test, Y_pred, average='micro')  # or 'macro'\n",
    "        f1_macro = f1_score(Y_test, Y_pred, average='macro')\n",
    "        f1_scores.append({'f1_micro': f1_micro, 'f1_macro': f1_macro})\n",
    "        f1_scores_df = pd.DataFrame(f1_scores)\n",
    "\n",
    "        # Compute confusion matrices per label\n",
    "        cm = multilabel_confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "        rows = []\n",
    "\n",
    "        # Show false posotives and false negatives per label\n",
    "        for i, label in enumerate(label_names):\n",
    "            tn, fp, fn, tp = cm[i].ravel()\n",
    "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "            fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "            tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "            fold_metrics.append({\n",
    "                'Model': model_name,\n",
    "                'Fold': len(fold_metrics) // len(label_names) + 1,\n",
    "                'Label': label,\n",
    "                'F1_micro': f1_micro,\n",
    "                'F1_macro': f1_macro,\n",
    "                'FPR': fpr,\n",
    "                'FNR': fnr,\n",
    "                'TPR': tpr,\n",
    "                'TNR': tnr,\n",
    "                'TP': tp,\n",
    "                'FP': fp,\n",
    "                'FN': fn,\n",
    "                'TN': tn\n",
    "            })\n",
    "\n",
    "        # Convert per-fold metrics to DataFrame\n",
    "        fold_metrics_df = pd.DataFrame(fold_metrics)\n",
    "\n",
    "        # Aggregate over folds: mean per model and label\n",
    "        summary_df = fold_metrics_df.groupby(['Model', 'Label']).agg({\n",
    "            'F1_micro': 'mean',\n",
    "            'F1_macro': 'mean',\n",
    "            'FPR': 'mean',\n",
    "            'FNR': 'mean',\n",
    "            'TPR': 'mean',\n",
    "            'TNR': 'mean',\n",
    "        }).reset_index()\n",
    "\n",
    "        # Append summary to global results list\n",
    "        results.append(summary_df)\n",
    "\n",
    "        print(f\"Average F1 (micro): {np.mean(f1_scores_df.f1_micro):.4f}\")\n",
    "        print(f\"Average F1 (macro): {np.mean(f1_scores_df.f1_macro):.4f}\")\n",
    "        print(classification_report(Y_test, Y_pred, target_names=label_names))\n",
    "\n",
    "\n",
    "    # Combine all models into one DataFrame\n",
    "    final_results_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    final_results_df.to_csv('../out/model_performance_summary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for best parameters for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      " Best parameters (global) for base LR: {'estimator__C': 1.0, 'estimator__penalty': 'l1', 'estimator__solver': 'liblinear'}\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      " Best parameters (global) for balanced LR: {'estimator__C': 0.1, 'estimator__penalty': 'l1', 'estimator__solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Logistic Regression Hyperparameter Grid\n",
    "param_grid_lr = {\n",
    "    'estimator__C': [0.1, 1.0, 10.0],\n",
    "    'estimator__penalty': ['l1', 'l2'],\n",
    "    'estimator__solver': ['liblinear']  # Only solver supporting l1\n",
    "}\n",
    "\n",
    "# Model wrapper\n",
    "base_lr = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "balanced_lr = OneVsRestClassifier(LogisticRegression(class_weight='balanced', max_iter=1000))\n",
    "\n",
    "# For storing metrics\n",
    "# all_f1_scores = []\n",
    "\n",
    "# # Grid Search for base LR\n",
    "# for fold_idx, (train_idx, test_idx) in enumerate(stratifier.split(X, Y)):\n",
    "#     print(f\"\\n Fold {fold_idx + 1}/{n_splits}\")\n",
    "\n",
    "#     X_train, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "#     # Grid search\n",
    "#     grid_search = GridSearchCV(base_lr, param_grid_lr, cv=3, scoring='f1_micro', verbose=1, n_jobs=-1)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     print(f\"Best params for Base LR: {grid_search.best_params_}\")\n",
    "\n",
    "#     y_pred = best_model.predict(X_test)\n",
    "#     y_pred = y_pred.toarray() if hasattr(y_pred, \"toarray\") else y_pred\n",
    "\n",
    "#     f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "#     all_f1_scores.append(f1)\n",
    "\n",
    "#     print(f\"F1 Micro: {f1:.4f}\")\n",
    "#     print(classification_report(y_test, y_pred, target_names=label_names))\n",
    "\n",
    "# print(f\"\\n Average F1 Micro over all folds: {np.mean(all_f1_scores):.4f}\")\n",
    "\n",
    "# # Grid Search for base LR\n",
    "# for fold_idx, (train_idx, test_idx) in enumerate(stratifier.split(X, Y)):\n",
    "#     print(f\"\\n Fold {fold_idx + 1}/{n_splits}\")\n",
    "\n",
    "#     X_train, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "#     # Grid search\n",
    "#     grid_search = GridSearchCV(balanced_lr, param_grid_lr, cv=3, scoring='f1_micro', verbose=1, n_jobs=-1)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     print(f\"Best params for Balanced LR: {grid_search.best_params_}\")\n",
    "\n",
    "#     y_pred = best_model.predict(X_test)\n",
    "#     y_pred = y_pred.toarray() if hasattr(y_pred, \"toarray\") else y_pred\n",
    "\n",
    "#     f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "#     all_f1_scores.append(f1)\n",
    "\n",
    "#     print(f\"F1 Micro: {f1:.4f}\")\n",
    "#     print(classification_report(y_test, y_pred, target_names=label_names))\n",
    "\n",
    "# print(f\"\\n Average F1 Micro over all folds: {np.mean(all_f1_scores):.4f}\")\n",
    "\n",
    "grid_search_base_lr = GridSearchCV(\n",
    "    OneVsRestClassifier(LogisticRegression(max_iter=1000)),\n",
    "    param_grid_lr,\n",
    "    cv=stratifier,\n",
    "    scoring='f1_micro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_base_lr.fit(X, Y)\n",
    "print(\" Best parameters (global) for base LR:\", grid_search_base_lr.best_params_)\n",
    "\n",
    "grid_search_balanced_lr = GridSearchCV(\n",
    "    OneVsRestClassifier(LogisticRegression(class_weight='balanced', max_iter=1000)),\n",
    "    param_grid_lr,\n",
    "    cv=stratifier,\n",
    "    scoring='f1_micro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_balanced_lr.fit(X, Y)\n",
    "print(\" Best parameters (global) for balanced LR:\", grid_search_balanced_lr.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for Random Forest Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 126 candidates, totalling 630 fits\n",
      " Best parameters (global) for Random Forest: {'estimator__max_depth': None, 'estimator__min_samples_split': 2, 'estimator__n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest Hyperparameter Grid\n",
    "param_grid_rf = {\n",
    "    'estimator__n_estimators': [50, 75, 100, 125, 150, 175, 200],\n",
    "    'estimator__max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'estimator__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Model wrapper\n",
    "base_rf = OneVsRestClassifier(RandomForestClassifier())\n",
    "\n",
    "# # For storing metrics\n",
    "# all_f1_scores = []\n",
    "\n",
    "# # Grid Search for base RF\n",
    "# for fold_idx, (train_idx, test_idx) in enumerate(stratifier.split(X, Y)):\n",
    "#     print(f\"\\n Fold {fold_idx + 1}/{n_splits}\")\n",
    "\n",
    "#     X_train, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "#     # Grid search\n",
    "#     grid_search = GridSearchCV(base_rf, param_grid_rf, cv=3, scoring='f1_micro', verbose=1, n_jobs=-1)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     print(f\"Best params for Random Forest: {grid_search.best_params_}\")\n",
    "\n",
    "#     y_pred = best_model.predict(X_test)\n",
    "#     y_pred = y_pred.toarray() if hasattr(y_pred, \"toarray\") else y_pred\n",
    "\n",
    "#     f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "#     all_f1_scores.append(f1)\n",
    "\n",
    "#     print(f\"F1 Micro: {f1:.4f}\")\n",
    "#     print(classification_report(y_test, y_pred, target_names=label_names))\n",
    "\n",
    "# print(f\"\\n Average F1 Micro over all folds: {np.mean(all_f1_scores):.4f}\")\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    OneVsRestClassifier(RandomForestClassifier()),\n",
    "    param_grid_rf,\n",
    "    cv=stratifier,\n",
    "    scoring='f1_micro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_rf.fit(X, Y)\n",
    "print(\" Best parameters (global) for Random Forest:\", grid_search_rf.best_params_)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
