{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model (Bag of Words approach)\n",
    "\n",
    "Naive model using a non-contextual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\felix\\Documents\\GIT\\Hertie\\PACT-ML\\modules\n",
      "Added to path: c:\\Users\\felix\\Documents\\GIT\\Hertie\\PACT-ML\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Add the parent directory to the path if needed\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "print(f\"Added to path: {parent_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472 paragraphs have multiple codings.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyreadr import read_r\n",
    "from modules.helpers.validity_check import fuzzy_match_report_key\n",
    "\n",
    "para_data = pd.read_csv(\"..\\data\\PACT_paragraphs_training.csv\")\n",
    "\n",
    "report_data = pd.read_csv(\"..\\data\\paragraphs.csv\", sep=';')\n",
    "\n",
    "report_data[\"matchingKey\"] = report_data[\"report_namePKO\"].str.replace('/', '_')\n",
    "\n",
    "# reduced data to 7 target categories with most codings\n",
    "target_categories = [\n",
    "    \"PoliceReform\",\n",
    "    \"Operations_PatrolsInterventions\",\n",
    "    \"StateAdministration\",\n",
    "    \"RefugeeAssistance\",\n",
    "    \"ElectionAssistance\",\n",
    "    \"LegalReform\",\n",
    "    \"CivilSocietyAssistance\"\n",
    "]\n",
    "\n",
    "report_data = report_data[[\"matchingKey\", \"paragraphNumber\"] + target_categories]\n",
    "\n",
    "report_data[target_categories] = report_data[target_categories].map(lambda x: isinstance(x, str))\n",
    "\n",
    "# check if a paragraph led to two codings\n",
    "multi_coded = report_data.copy()\n",
    "multi_coded[\"num_labels\"] = report_data[target_categories].sum(axis=1)\n",
    "\n",
    "multi_coded = multi_coded[multi_coded[\"num_labels\"] > 1]\n",
    "\n",
    "print(f\"{len(multi_coded)} paragraphs have multiple codings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# number of coded paragraphs in PACT2.0 that have text in the training data\u001b[39;00m\n",
      "\u001b[0;32m      2\u001b[0m merged \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\n",
      "\u001b[0;32m      3\u001b[0m     report_data[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatchingKey\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparagraphNumber\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n",
      "\u001b[0;32m      4\u001b[0m     para_data[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatchingKey\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparagraphNumber\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m      7\u001b[0m     indicator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;32m---> 10\u001b[0m \u001b[43mmerged_data\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/merged_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;32m     12\u001b[0m matching_pairs \u001b[38;5;241m=\u001b[39m (merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_merge\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRows matching on both matchingKey and paragraphNumber: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmatching_pairs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# number of coded paragraphs in PACT2.0 that have text in the training data\n",
    "merged = pd.merge(\n",
    "    report_data[[\"matchingKey\", \"paragraphNumber\"]],\n",
    "    para_data[[\"matchingKey\", \"paragraphNumber\"]],\n",
    "    on=[\"matchingKey\", \"paragraphNumber\"],\n",
    "    how=\"left\",\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "matching_pairs = (merged[\"_merge\"] == \"both\").sum()\n",
    "print(f\"Rows matching on both matchingKey and paragraphNumber: {matching_pairs}\")\n",
    "\n",
    "# number of multi-label paragraphs in PACT2.0 that have text in the training data\n",
    "merged_multi = pd.merge(\n",
    "    multi_coded[[\"matchingKey\", \"paragraphNumber\"]],\n",
    "    para_data[[\"matchingKey\", \"paragraphNumber\"]],\n",
    "    on=[\"matchingKey\", \"paragraphNumber\"],\n",
    "    how=\"left\",\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "matching_pairs_multi = (merged_multi[\"_merge\"] == \"both\").sum()\n",
    "print(f\"Rows matching on both matchingKey and paragraphNumber for multi-coded paragraphs: {matching_pairs_multi}\")\n",
    "\n",
    "# number of paragraphs for which parsing failed\n",
    "failed_parsing = report_data[\"matchingKey\"].isin(para_data[\"matchingKey\"]).sum() - matching_pairs\n",
    "print(f\"Number of coded paragraphs for which text parsing failed: {failed_parsing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of merged data: (1819, 12)\n",
      "Number of features for Count Vectorizer: 2989\n",
      "Number of features for TF-IDF: 2989\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "merged_data = pd.merge(\n",
    "    para_data,\n",
    "    report_data,\n",
    "    on=[\"matchingKey\", \"paragraphNumber\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "merged_data.to_csv(\"../data/merged_data.csv\", index=False)\n",
    "\n",
    "# Check the shape of the merged data\n",
    "print(f\"Shape of merged data: {merged_data.shape}\")\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=5, max_df=0.95)\n",
    "X = vectorizer.fit_transform(merged_data[\"paragraph\"])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.95)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(merged_data['paragraph'])\n",
    "\n",
    "df_bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(f\"Number of features for Count Vectorizer: {df_bow.shape[1]}\")\n",
    "print(f\"Number of features for TF-IDF: {df_tfidf.shape[1]}\")\n",
    "\n",
    "# MAYBE USE STATIFIED K-FOLD CV\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Stratification K-fold CV\n",
    "\n",
    "To make the most of our limited multi-label data, we use Iterative Stratification K-fold CV across all our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "\n",
    "Y = merged_data[target_categories].values\n",
    "\n",
    "# Set up Iterative Stratification\n",
    "n_splits = 5\n",
    "stratifier = IterativeStratification(n_splits=n_splits, order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Class Distribution\n",
      "\n",
      "| Category | Count | Proportion | Percentage |\n",
      "|----------|-------|------------|------------|\n",
      "| PoliceReform | 525 | 0.2886 | 28.86% |\n",
      "| Operations_PatrolsInterventions | 151 | 0.0830 | 8.30% |\n",
      "| StateAdministration | 219 | 0.1204 | 12.04% |\n",
      "| RefugeeAssistance | 103 | 0.0566 | 5.66% |\n",
      "| ElectionAssistance | 112 | 0.0616 | 6.16% |\n",
      "| LegalReform | 129 | 0.0709 | 7.09% |\n",
      "| CivilSocietyAssistance | 161 | 0.0885 | 8.85% |\n",
      "| **Total** | 1819 | 1.0000 | 100.00% |\n",
      "\n",
      "**Imbalance ratio (most frequent / least frequent)**: 5.10\n"
     ]
    }
   ],
   "source": [
    "y = merged_data[target_categories]\n",
    "class_counts = y.sum()\n",
    "total_samples = len(y)\n",
    "class_distribution = class_counts / total_samples\n",
    "\n",
    "# Create a markdown table\n",
    "print(\"## Class Distribution\\n\")\n",
    "print(\"| Category | Count | Proportion | Percentage |\")\n",
    "print(\"|----------|-------|------------|------------|\")\n",
    "for category, proportion in class_distribution.items():\n",
    "    count = class_counts[category]\n",
    "    print(f\"| {category} | {count} | {proportion:.4f} | {proportion*100:.2f}% |\")\n",
    "\n",
    "# Add summary row\n",
    "print(f\"| **Total** | {total_samples} | 1.0000 | 100.00% |\")\n",
    "\n",
    "# Calculate and add imbalance ratio\n",
    "most_frequent = class_distribution.max()\n",
    "least_frequent = class_distribution.min()\n",
    "imbalance_ratio = most_frequent / least_frequent\n",
    "print(f\"\\n**Imbalance ratio (most frequent / least frequent)**: {imbalance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 37 category columns\n",
      "## Class Distribution in paragraphs.csv (All Categories)\n",
      "\n",
      "| Category | Count | Proportion | Percentage |\n",
      "|----------|-------|------------|------------|\n",
      "| PoliceReform | 1232 | 0.2532 | 25.32% |\n",
      "| Operations_PatrolsInterventions | 587 | 0.1207 | 12.07% |\n",
      "| StateAdministration | 581 | 0.1194 | 11.94% |\n",
      "| HumanRights | 450 | 0.0925 | 9.25% |\n",
      "| JusticeSectorReform | 435 | 0.0894 | 8.94% |\n",
      "| Demilitarization | 350 | 0.0719 | 7.19% |\n",
      "| RefugeeAssistance | 312 | 0.0641 | 6.41% |\n",
      "| ElectionAssistance | 265 | 0.0545 | 5.45% |\n",
      "| BorderControl | 251 | 0.0516 | 5.16% |\n",
      "| MilitaryReform | 247 | 0.0508 | 5.08% |\n",
      "| LegalReform | 235 | 0.0483 | 4.83% |\n",
      "| CivilSocietyAssistance | 232 | 0.0477 | 4.77% |\n",
      "| PrisonReform | 228 | 0.0469 | 4.69% |\n",
      "| HumanitarianRelief | 222 | 0.0456 | 4.56% |\n",
      "| Gender | 209 | 0.0430 | 4.30% |\n",
      "| PartyAssistance | 178 | 0.0366 | 3.66% |\n",
      "| DemocraticInstitutions | 144 | 0.0296 | 2.96% |\n",
      "| SexualViolence | 142 | 0.0292 | 2.92% |\n",
      "| ControlSALW | 133 | 0.0273 | 2.73% |\n",
      "| Operations_UseOfForce | 132 | 0.0271 | 2.71% |\n",
      "| TransitionalJustice | 123 | 0.0253 | 2.53% |\n",
      "| PublicHealth | 109 | 0.0224 | 2.24% |\n",
      "| LocalReconciliation | 102 | 0.0210 | 2.10% |\n",
      "| ElectoralSecurity | 99 | 0.0203 | 2.03% |\n",
      "| EconomicDevelopment | 98 | 0.0201 | 2.01% |\n",
      "| ChildRights | 96 | 0.0197 | 1.97% |\n",
      "| DisarmamentDemobilization | 95 | 0.0195 | 1.95% |\n",
      "| Media | 80 | 0.0164 | 1.64% |\n",
      "| Demining | 79 | 0.0162 | 1.62% |\n",
      "| National_Reconciliation | 77 | 0.0158 | 1.58% |\n",
      "| CivilianProtection | 61 | 0.0125 | 1.25% |\n",
      "| Reintegration | 54 | 0.0111 | 1.11% |\n",
      "| Resources | 38 | 0.0078 | 0.78% |\n",
      "| VoterEducation | 30 | 0.0062 | 0.62% |\n",
      "| StateAuthority | 23 | 0.0047 | 0.47% |\n",
      "| PowerSharing | 9 | 0.0018 | 0.18% |\n",
      "| ArmsEmbargo | 0 | 0.0000 | 0.00% |\n",
      "| **Total Documents** | 4865 | - | - |\n",
      "\n",
      "**Imbalance ratio (most frequent / least frequent)**: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-17>:45: RuntimeWarning: divide by zero encountered in scalar divide\n"
     ]
    }
   ],
   "source": [
    "full_PACT = pd.read_csv(\"..\\data\\paragraphs.csv\", sep=';')\n",
    "\n",
    "exclude_columns = [\"report_namePKO\", \"paragraphNumber\", \"paragraph_ID\"]\n",
    "potential_categories = [col for col in full_PACT.columns if col not in exclude_columns]\n",
    "\n",
    "# Check which of these columns contain boolean-like data\n",
    "all_categories = []\n",
    "for col in potential_categories:\n",
    "    # Check if column contains only boolean values, or strings, or NaNs\n",
    "    unique_values = full_PACT[col].dropna().unique()\n",
    "    if all(isinstance(x, bool) for x in unique_values) or \\\n",
    "       all(isinstance(x, str) for x in unique_values) or \\\n",
    "       len(unique_values) <= 2:  # Assuming binary categories\n",
    "        all_categories.append(col)\n",
    "\n",
    "print(f\"Detected {len(all_categories)} category columns\")\n",
    "\n",
    "# Convert string values to boolean if needed\n",
    "for category in all_categories:\n",
    "    full_PACT[category] = full_PACT[category].map(lambda x: isinstance(x, str) if pd.notna(x) else False)\n",
    "\n",
    "# Calculate counts and distribution\n",
    "class_counts = full_PACT[all_categories].sum()\n",
    "total_samples = len(full_PACT)\n",
    "class_distribution = class_counts / total_samples\n",
    "\n",
    "# Sort categories by frequency for better readability\n",
    "sorted_categories = class_counts.sort_values(ascending=False).index.tolist()\n",
    "\n",
    "# Create a markdown table\n",
    "print(\"## Class Distribution in paragraphs.csv (All Categories)\\n\")\n",
    "print(\"| Category | Count | Proportion | Percentage |\")\n",
    "print(\"|----------|-------|------------|------------|\")\n",
    "for category in sorted_categories:\n",
    "    count = class_counts[category]\n",
    "    proportion = class_distribution[category]\n",
    "    print(f\"| {category} | {count} | {proportion:.4f} | {proportion*100:.2f}% |\")\n",
    "\n",
    "# Add summary row\n",
    "print(f\"| **Total Documents** | {total_samples} | - | - |\")\n",
    "\n",
    "# Calculate and add imbalance ratio\n",
    "most_frequent = class_distribution.max()\n",
    "least_frequent = class_distribution.min()\n",
    "imbalance_ratio = most_frequent / least_frequent\n",
    "print(f\"\\n**Imbalance ratio (most frequent / least frequent)**: {imbalance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Model + Random Forest using BoW\n",
    "Now, we can finally calculate our first two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Logistic Regression ---\n",
      "Average F1 (micro): 0.6265\n",
      "Average F1 (macro): 0.5678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.82      0.70      0.76       105\n",
      "Operations_PatrolsInterventions       0.87      0.43      0.58        30\n",
      "            StateAdministration       0.68      0.59      0.63        44\n",
      "              RefugeeAssistance       0.92      0.52      0.67        21\n",
      "             ElectionAssistance       0.83      0.43      0.57        23\n",
      "                    LegalReform       0.58      0.27      0.37        26\n",
      "         CivilSocietyAssistance       0.62      0.64      0.63        33\n",
      "\n",
      "                      micro avg       0.76      0.57      0.65       282\n",
      "                      macro avg       0.76      0.51      0.60       282\n",
      "                   weighted avg       0.77      0.57      0.65       282\n",
      "                    samples avg       0.38      0.34      0.35       282\n",
      "\n",
      "Average F1 (micro): 0.6269\n",
      "Average F1 (macro): 0.5685\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.81      0.71      0.76       105\n",
      "Operations_PatrolsInterventions       0.69      0.58      0.63        31\n",
      "            StateAdministration       0.70      0.52      0.60        44\n",
      "              RefugeeAssistance       0.79      0.55      0.65        20\n",
      "             ElectionAssistance       0.80      0.70      0.74        23\n",
      "                    LegalReform       0.78      0.54      0.64        26\n",
      "         CivilSocietyAssistance       0.57      0.41      0.47        32\n",
      "\n",
      "                      micro avg       0.75      0.60      0.67       281\n",
      "                      macro avg       0.73      0.57      0.64       281\n",
      "                   weighted avg       0.74      0.60      0.67       281\n",
      "                    samples avg       0.39      0.38      0.38       281\n",
      "\n",
      "Average F1 (micro): 0.6271\n",
      "Average F1 (macro): 0.5689\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.85      0.69      0.76       105\n",
      "Operations_PatrolsInterventions       1.00      0.53      0.70        30\n",
      "            StateAdministration       0.87      0.47      0.61        43\n",
      "              RefugeeAssistance       0.91      0.48      0.62        21\n",
      "             ElectionAssistance       0.65      0.68      0.67        22\n",
      "                    LegalReform       0.62      0.38      0.48        26\n",
      "         CivilSocietyAssistance       0.52      0.38      0.44        32\n",
      "\n",
      "                      micro avg       0.79      0.56      0.65       279\n",
      "                      macro avg       0.77      0.51      0.61       279\n",
      "                   weighted avg       0.80      0.56      0.65       279\n",
      "                    samples avg       0.37      0.34      0.35       279\n",
      "\n",
      "Average F1 (micro): 0.6274\n",
      "Average F1 (macro): 0.5693\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.77      0.71      0.74       105\n",
      "Operations_PatrolsInterventions       0.74      0.57      0.64        30\n",
      "            StateAdministration       0.73      0.55      0.62        44\n",
      "              RefugeeAssistance       0.81      0.65      0.72        20\n",
      "             ElectionAssistance       0.72      0.59      0.65        22\n",
      "                    LegalReform       0.65      0.42      0.51        26\n",
      "         CivilSocietyAssistance       0.73      0.34      0.47        32\n",
      "\n",
      "                      micro avg       0.75      0.59      0.66       279\n",
      "                      macro avg       0.74      0.55      0.62       279\n",
      "                   weighted avg       0.74      0.59      0.65       279\n",
      "                    samples avg       0.40      0.38      0.38       279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.6279\n",
      "Average F1 (macro): 0.5699\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.84      0.71      0.77       105\n",
      "Operations_PatrolsInterventions       0.89      0.53      0.67        30\n",
      "            StateAdministration       0.78      0.57      0.66        44\n",
      "              RefugeeAssistance       1.00      0.43      0.60        21\n",
      "             ElectionAssistance       0.92      0.55      0.69        22\n",
      "                    LegalReform       0.86      0.24      0.38        25\n",
      "         CivilSocietyAssistance       0.78      0.56      0.65        32\n",
      "\n",
      "                      micro avg       0.84      0.58      0.69       279\n",
      "                      macro avg       0.87      0.51      0.63       279\n",
      "                   weighted avg       0.85      0.58      0.67       279\n",
      "                    samples avg       0.38      0.36      0.37       279\n",
      "\n",
      "\n",
      "--- Balanced Logistic Regression ---\n",
      "Average F1 (micro): 0.6283\n",
      "Average F1 (macro): 0.5706\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.76      0.73      0.75       105\n",
      "Operations_PatrolsInterventions       0.66      0.61      0.63        31\n",
      "            StateAdministration       0.65      0.64      0.64        44\n",
      "              RefugeeAssistance       0.83      0.50      0.62        20\n",
      "             ElectionAssistance       0.74      0.74      0.74        23\n",
      "                    LegalReform       0.65      0.65      0.65        26\n",
      "         CivilSocietyAssistance       0.56      0.59      0.58        32\n",
      "\n",
      "                      micro avg       0.70      0.67      0.68       281\n",
      "                      macro avg       0.69      0.64      0.66       281\n",
      "                   weighted avg       0.70      0.67      0.68       281\n",
      "                    samples avg       0.42      0.41      0.41       281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.6289\n",
      "Average F1 (macro): 0.5715\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.73      0.75      0.74       105\n",
      "Operations_PatrolsInterventions       0.71      0.80      0.75        30\n",
      "            StateAdministration       0.56      0.64      0.60        44\n",
      "              RefugeeAssistance       0.79      0.52      0.63        21\n",
      "             ElectionAssistance       0.87      0.57      0.68        23\n",
      "                    LegalReform       0.59      0.62      0.60        26\n",
      "         CivilSocietyAssistance       0.79      0.67      0.72        33\n",
      "\n",
      "                      micro avg       0.70      0.68      0.69       282\n",
      "                      macro avg       0.72      0.65      0.68       282\n",
      "                   weighted avg       0.71      0.68      0.69       282\n",
      "                    samples avg       0.43      0.42      0.42       282\n",
      "\n",
      "Average F1 (micro): 0.6293\n",
      "Average F1 (macro): 0.5723\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.83      0.67      0.74       105\n",
      "Operations_PatrolsInterventions       0.75      0.60      0.67        30\n",
      "            StateAdministration       0.68      0.63      0.65        43\n",
      "              RefugeeAssistance       0.73      0.80      0.76        20\n",
      "             ElectionAssistance       0.59      0.73      0.65        22\n",
      "                    LegalReform       0.60      0.58      0.59        26\n",
      "         CivilSocietyAssistance       0.55      0.56      0.55        32\n",
      "\n",
      "                      micro avg       0.71      0.65      0.68       278\n",
      "                      macro avg       0.67      0.65      0.66       278\n",
      "                   weighted avg       0.72      0.65      0.68       278\n",
      "                    samples avg       0.42      0.41      0.40       278\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.6299\n",
      "Average F1 (macro): 0.5733\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.78      0.70      0.74       105\n",
      "Operations_PatrolsInterventions       0.67      0.73      0.70        30\n",
      "            StateAdministration       0.63      0.66      0.64        44\n",
      "              RefugeeAssistance       0.81      0.62      0.70        21\n",
      "             ElectionAssistance       0.81      0.95      0.88        22\n",
      "                    LegalReform       0.62      0.38      0.48        26\n",
      "         CivilSocietyAssistance       0.62      0.81      0.70        32\n",
      "\n",
      "                      micro avg       0.71      0.69      0.70       280\n",
      "                      macro avg       0.71      0.69      0.69       280\n",
      "                   weighted avg       0.72      0.69      0.70       280\n",
      "                    samples avg       0.42      0.42      0.41       280\n",
      "\n",
      "Average F1 (micro): 0.6304\n",
      "Average F1 (macro): 0.5741\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.75      0.76      0.76       105\n",
      "Operations_PatrolsInterventions       0.70      0.63      0.67        30\n",
      "            StateAdministration       0.81      0.66      0.72        44\n",
      "              RefugeeAssistance       0.71      0.81      0.76        21\n",
      "             ElectionAssistance       0.79      0.68      0.73        22\n",
      "                    LegalReform       0.55      0.48      0.51        25\n",
      "         CivilSocietyAssistance       0.53      0.53      0.53        32\n",
      "\n",
      "                      micro avg       0.71      0.68      0.69       279\n",
      "                      macro avg       0.69      0.65      0.67       279\n",
      "                   weighted avg       0.71      0.68      0.69       279\n",
      "                    samples avg       0.43      0.43      0.42       279\n",
      "\n",
      "\n",
      "--- Logistic Regression (hyperparameter optimized) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.6309\n",
      "Average F1 (macro): 0.5749\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.76      0.70      0.73       105\n",
      "Operations_PatrolsInterventions       0.79      0.63      0.70        30\n",
      "            StateAdministration       0.64      0.63      0.64        43\n",
      "              RefugeeAssistance       0.92      0.57      0.71        21\n",
      "             ElectionAssistance       0.94      0.73      0.82        22\n",
      "                    LegalReform       0.69      0.42      0.52        26\n",
      "         CivilSocietyAssistance       0.65      0.67      0.66        33\n",
      "\n",
      "                      micro avg       0.74      0.64      0.69       280\n",
      "                      macro avg       0.77      0.62      0.68       280\n",
      "                   weighted avg       0.75      0.64      0.69       280\n",
      "                    samples avg       0.40      0.37      0.38       280\n",
      "\n",
      "Average F1 (micro): 0.6312\n",
      "Average F1 (macro): 0.5755\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.79      0.73      0.76       105\n",
      "Operations_PatrolsInterventions       0.69      0.67      0.68        30\n",
      "            StateAdministration       0.66      0.52      0.58        44\n",
      "              RefugeeAssistance       0.81      0.85      0.83        20\n",
      "             ElectionAssistance       0.80      0.73      0.76        22\n",
      "                    LegalReform       0.60      0.35      0.44        26\n",
      "         CivilSocietyAssistance       0.45      0.44      0.44        32\n",
      "\n",
      "                      micro avg       0.71      0.63      0.67       279\n",
      "                      macro avg       0.68      0.61      0.64       279\n",
      "                   weighted avg       0.70      0.63      0.66       279\n",
      "                    samples avg       0.41      0.40      0.40       279\n",
      "\n",
      "Average F1 (micro): 0.6316\n",
      "Average F1 (macro): 0.5762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.73      0.70      0.71       105\n",
      "Operations_PatrolsInterventions       0.75      0.58      0.65        31\n",
      "            StateAdministration       0.68      0.59      0.63        44\n",
      "              RefugeeAssistance       0.83      0.90      0.86        21\n",
      "             ElectionAssistance       0.76      0.57      0.65        23\n",
      "                    LegalReform       0.71      0.40      0.51        25\n",
      "         CivilSocietyAssistance       0.73      0.50      0.59        32\n",
      "\n",
      "                      micro avg       0.74      0.62      0.67       281\n",
      "                      macro avg       0.74      0.61      0.66       281\n",
      "                   weighted avg       0.73      0.62      0.67       281\n",
      "                    samples avg       0.41      0.40      0.40       281\n",
      "\n",
      "Average F1 (micro): 0.6319\n",
      "Average F1 (macro): 0.5770\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.72      0.70      0.71       105\n",
      "Operations_PatrolsInterventions       0.91      0.70      0.79        30\n",
      "            StateAdministration       0.64      0.48      0.55        44\n",
      "              RefugeeAssistance       0.72      0.65      0.68        20\n",
      "             ElectionAssistance       0.80      0.70      0.74        23\n",
      "                    LegalReform       0.93      0.50      0.65        26\n",
      "         CivilSocietyAssistance       0.65      0.53      0.59        32\n",
      "\n",
      "                      micro avg       0.74      0.62      0.68       280\n",
      "                      macro avg       0.77      0.61      0.67       280\n",
      "                   weighted avg       0.75      0.62      0.67       280\n",
      "                    samples avg       0.39      0.38      0.38       280\n",
      "\n",
      "Average F1 (micro): 0.6318\n",
      "Average F1 (macro): 0.5771\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.76      0.62      0.68       105\n",
      "Operations_PatrolsInterventions       0.87      0.67      0.75        30\n",
      "            StateAdministration       0.65      0.55      0.59        44\n",
      "              RefugeeAssistance       0.76      0.62      0.68        21\n",
      "             ElectionAssistance       0.50      0.41      0.45        22\n",
      "                    LegalReform       0.46      0.42      0.44        26\n",
      "         CivilSocietyAssistance       0.67      0.44      0.53        32\n",
      "\n",
      "                      micro avg       0.69      0.56      0.62       280\n",
      "                      macro avg       0.67      0.53      0.59       280\n",
      "                   weighted avg       0.69      0.56      0.62       280\n",
      "                    samples avg       0.35      0.34      0.33       280\n",
      "\n",
      "\n",
      "--- Balanced Logistic Regression (hyperparameter optimized) ---\n",
      "Average F1 (micro): 0.6321\n",
      "Average F1 (macro): 0.5777\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.76      0.78      0.77       105\n",
      "Operations_PatrolsInterventions       0.56      0.77      0.65        30\n",
      "            StateAdministration       0.43      0.80      0.56        44\n",
      "              RefugeeAssistance       0.69      0.90      0.78        20\n",
      "             ElectionAssistance       0.63      0.77      0.69        22\n",
      "                    LegalReform       0.40      0.76      0.52        25\n",
      "         CivilSocietyAssistance       0.47      0.91      0.62        32\n",
      "\n",
      "                      micro avg       0.57      0.80      0.66       278\n",
      "                      macro avg       0.56      0.81      0.66       278\n",
      "                   weighted avg       0.60      0.80      0.68       278\n",
      "                    samples avg       0.44      0.49      0.45       278\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.6327\n",
      "Average F1 (macro): 0.5786\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.74      0.85      0.79       105\n",
      "Operations_PatrolsInterventions       0.61      0.81      0.69        31\n",
      "            StateAdministration       0.52      0.82      0.64        44\n",
      "              RefugeeAssistance       0.75      0.86      0.80        21\n",
      "             ElectionAssistance       0.55      0.78      0.64        23\n",
      "                    LegalReform       0.53      0.92      0.68        26\n",
      "         CivilSocietyAssistance       0.51      0.88      0.64        32\n",
      "\n",
      "                      micro avg       0.61      0.84      0.71       282\n",
      "                      macro avg       0.60      0.84      0.70       282\n",
      "                   weighted avg       0.63      0.84      0.72       282\n",
      "                    samples avg       0.47      0.51      0.47       282\n",
      "\n",
      "Average F1 (micro): 0.6329\n",
      "Average F1 (macro): 0.5792\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.73      0.73      0.73       105\n",
      "Operations_PatrolsInterventions       0.53      0.80      0.64        30\n",
      "            StateAdministration       0.45      0.79      0.58        43\n",
      "              RefugeeAssistance       0.61      0.95      0.74        21\n",
      "             ElectionAssistance       0.58      0.78      0.67        23\n",
      "                    LegalReform       0.56      0.73      0.63        26\n",
      "         CivilSocietyAssistance       0.50      0.75      0.60        32\n",
      "\n",
      "                      micro avg       0.58      0.77      0.66       280\n",
      "                      macro avg       0.57      0.79      0.66       280\n",
      "                   weighted avg       0.60      0.77      0.67       280\n",
      "                    samples avg       0.43      0.48      0.44       280\n",
      "\n",
      "Average F1 (micro): 0.6333\n",
      "Average F1 (macro): 0.5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.72      0.72      0.72       105\n",
      "Operations_PatrolsInterventions       0.58      0.87      0.69        30\n",
      "            StateAdministration       0.46      0.70      0.56        44\n",
      "              RefugeeAssistance       0.76      0.80      0.78        20\n",
      "             ElectionAssistance       0.56      0.86      0.68        22\n",
      "                    LegalReform       0.57      0.81      0.67        26\n",
      "         CivilSocietyAssistance       0.52      0.88      0.65        32\n",
      "\n",
      "                      micro avg       0.60      0.78      0.68       279\n",
      "                      macro avg       0.60      0.81      0.68       279\n",
      "                   weighted avg       0.62      0.78      0.68       279\n",
      "                    samples avg       0.44      0.47      0.44       279\n",
      "\n",
      "Average F1 (micro): 0.6336\n",
      "Average F1 (macro): 0.5808\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.76      0.77      0.76       105\n",
      "Operations_PatrolsInterventions       0.58      0.73      0.65        30\n",
      "            StateAdministration       0.37      0.80      0.50        44\n",
      "              RefugeeAssistance       0.75      0.86      0.80        21\n",
      "             ElectionAssistance       0.58      1.00      0.73        22\n",
      "                    LegalReform       0.49      0.85      0.62        26\n",
      "         CivilSocietyAssistance       0.55      0.88      0.67        33\n",
      "\n",
      "                      micro avg       0.57      0.81      0.67       281\n",
      "                      macro avg       0.58      0.84      0.68       281\n",
      "                   weighted avg       0.61      0.81      0.69       281\n",
      "                    samples avg       0.45      0.51      0.46       281\n",
      "\n",
      "\n",
      "--- Random Forest ---\n",
      "Average F1 (micro): 0.6324\n",
      "Average F1 (macro): 0.5790\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.93      0.59      0.72       105\n",
      "Operations_PatrolsInterventions       0.89      0.27      0.41        30\n",
      "            StateAdministration       1.00      0.18      0.31        44\n",
      "              RefugeeAssistance       0.89      0.38      0.53        21\n",
      "             ElectionAssistance       1.00      0.14      0.24        22\n",
      "                    LegalReform       0.75      0.12      0.20        26\n",
      "         CivilSocietyAssistance       0.50      0.03      0.06        32\n",
      "\n",
      "                      micro avg       0.91      0.33      0.49       280\n",
      "                      macro avg       0.85      0.24      0.35       280\n",
      "                   weighted avg       0.87      0.33      0.45       280\n",
      "                    samples avg       0.24      0.21      0.22       280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.6314\n",
      "Average F1 (macro): 0.5774\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.82      0.63      0.71       105\n",
      "Operations_PatrolsInterventions       0.91      0.32      0.48        31\n",
      "            StateAdministration       1.00      0.20      0.34        44\n",
      "              RefugeeAssistance       0.80      0.20      0.32        20\n",
      "             ElectionAssistance       0.78      0.32      0.45        22\n",
      "                    LegalReform       1.00      0.04      0.07        26\n",
      "         CivilSocietyAssistance       0.75      0.09      0.17        32\n",
      "\n",
      "                      micro avg       0.84      0.36      0.50       280\n",
      "                      macro avg       0.87      0.26      0.36       280\n",
      "                   weighted avg       0.86      0.36      0.46       280\n",
      "                    samples avg       0.25      0.23      0.24       280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.6307\n",
      "Average F1 (macro): 0.5763\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.91      0.59      0.72       105\n",
      "Operations_PatrolsInterventions       0.91      0.33      0.49        30\n",
      "            StateAdministration       0.85      0.25      0.39        44\n",
      "              RefugeeAssistance       0.75      0.30      0.43        20\n",
      "             ElectionAssistance       1.00      0.52      0.69        23\n",
      "                    LegalReform       1.00      0.08      0.14        26\n",
      "         CivilSocietyAssistance       0.43      0.09      0.15        33\n",
      "\n",
      "                      micro avg       0.88      0.38      0.53       281\n",
      "                      macro avg       0.84      0.31      0.43       281\n",
      "                   weighted avg       0.85      0.38      0.50       281\n",
      "                    samples avg       0.28      0.26      0.26       281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.6296\n",
      "Average F1 (macro): 0.5746\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.82      0.58      0.68       105\n",
      "Operations_PatrolsInterventions       0.85      0.37      0.51        30\n",
      "            StateAdministration       1.00      0.30      0.46        43\n",
      "              RefugeeAssistance       1.00      0.19      0.32        21\n",
      "             ElectionAssistance       1.00      0.17      0.30        23\n",
      "                    LegalReform       0.00      0.00      0.00        25\n",
      "         CivilSocietyAssistance       1.00      0.09      0.17        32\n",
      "\n",
      "                      micro avg       0.86      0.34      0.49       279\n",
      "                      macro avg       0.81      0.24      0.35       279\n",
      "                   weighted avg       0.83      0.34      0.45       279\n",
      "                    samples avg       0.25      0.23      0.24       279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.6289\n",
      "Average F1 (macro): 0.5731\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.89      0.69      0.77       105\n",
      "Operations_PatrolsInterventions       0.92      0.37      0.52        30\n",
      "            StateAdministration       0.80      0.18      0.30        44\n",
      "              RefugeeAssistance       0.75      0.14      0.24        21\n",
      "             ElectionAssistance       1.00      0.23      0.37        22\n",
      "                    LegalReform       1.00      0.15      0.27        26\n",
      "         CivilSocietyAssistance       1.00      0.09      0.17        32\n",
      "\n",
      "                      micro avg       0.89      0.38      0.53       280\n",
      "                      macro avg       0.91      0.26      0.38       280\n",
      "                   weighted avg       0.90      0.38      0.48       280\n",
      "                    samples avg       0.27      0.24      0.25       280\n",
      "\n",
      "\n",
      "--- Random Forest (hyperparameter optimized) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.6281\n",
      "Average F1 (macro): 0.5719\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.82      0.61      0.70       105\n",
      "Operations_PatrolsInterventions       1.00      0.39      0.56        31\n",
      "            StateAdministration       1.00      0.16      0.27        44\n",
      "              RefugeeAssistance       0.78      0.33      0.47        21\n",
      "             ElectionAssistance       0.80      0.35      0.48        23\n",
      "                    LegalReform       1.00      0.12      0.21        26\n",
      "         CivilSocietyAssistance       1.00      0.09      0.17        32\n",
      "\n",
      "                      micro avg       0.85      0.37      0.51       282\n",
      "                      macro avg       0.91      0.29      0.41       282\n",
      "                   weighted avg       0.90      0.37      0.48       282\n",
      "                    samples avg       0.26      0.23      0.24       282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.6275\n",
      "Average F1 (macro): 0.5709\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.87      0.69      0.77       105\n",
      "Operations_PatrolsInterventions       0.93      0.43      0.59        30\n",
      "            StateAdministration       0.82      0.20      0.33        44\n",
      "              RefugeeAssistance       0.88      0.35      0.50        20\n",
      "             ElectionAssistance       0.80      0.36      0.50        22\n",
      "                    LegalReform       0.50      0.04      0.07        26\n",
      "         CivilSocietyAssistance       0.71      0.15      0.25        33\n",
      "\n",
      "                      micro avg       0.85      0.41      0.55       280\n",
      "                      macro avg       0.79      0.32      0.43       280\n",
      "                   weighted avg       0.81      0.41      0.51       280\n",
      "                    samples avg       0.29      0.27      0.27       280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.6266\n",
      "Average F1 (macro): 0.5694\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.97      0.57      0.72       105\n",
      "Operations_PatrolsInterventions       1.00      0.43      0.60        30\n",
      "            StateAdministration       0.92      0.26      0.40        43\n",
      "              RefugeeAssistance       0.70      0.33      0.45        21\n",
      "             ElectionAssistance       1.00      0.18      0.31        22\n",
      "                    LegalReform       0.00      0.00      0.00        26\n",
      "         CivilSocietyAssistance       1.00      0.03      0.06        32\n",
      "\n",
      "                      micro avg       0.94      0.34      0.50       279\n",
      "                      macro avg       0.80      0.26      0.36       279\n",
      "                   weighted avg       0.86      0.34      0.46       279\n",
      "                    samples avg       0.25      0.23      0.24       279\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.6260\n",
      "Average F1 (macro): 0.5682\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.85      0.66      0.74       105\n",
      "Operations_PatrolsInterventions       0.84      0.53      0.65        30\n",
      "            StateAdministration       1.00      0.25      0.40        44\n",
      "              RefugeeAssistance       0.80      0.38      0.52        21\n",
      "             ElectionAssistance       1.00      0.13      0.23        23\n",
      "                    LegalReform       1.00      0.12      0.21        26\n",
      "         CivilSocietyAssistance       1.00      0.03      0.06        32\n",
      "\n",
      "                      micro avg       0.87      0.40      0.54       281\n",
      "                      macro avg       0.93      0.30      0.40       281\n",
      "                   weighted avg       0.91      0.40      0.49       281\n",
      "                    samples avg       0.27      0.25      0.26       281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 (micro): 0.6252\n",
      "Average F1 (macro): 0.5666\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                   PoliceReform       0.85      0.64      0.73       105\n",
      "Operations_PatrolsInterventions       0.90      0.30      0.45        30\n",
      "            StateAdministration       0.88      0.32      0.47        44\n",
      "              RefugeeAssistance       1.00      0.20      0.33        20\n",
      "             ElectionAssistance       1.00      0.14      0.24        22\n",
      "                    LegalReform       1.00      0.04      0.08        25\n",
      "         CivilSocietyAssistance       0.75      0.09      0.17        32\n",
      "\n",
      "                      micro avg       0.86      0.36      0.51       278\n",
      "                      macro avg       0.91      0.25      0.35       278\n",
      "                   weighted avg       0.88      0.36      0.47       278\n",
      "                    samples avg       0.25      0.22      0.23       278\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\felix\\miniconda3\\envs\\pact-ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, multilabel_confusion_matrix\n",
    "\n",
    "best_params_rf = {'estimator__max_depth': None, 'estimator__min_samples_split': 2, 'estimator__n_estimators': 75}\n",
    "\n",
    "rf_params = {k.replace('estimator__', ''): v for k, v in best_params_rf.items()}\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": OneVsRestClassifier(LogisticRegression(max_iter=1000)),\n",
    "    \"Balanced Logistic Regression\": OneVsRestClassifier(LogisticRegression(class_weight='balanced', max_iter=1000)),\n",
    "    \"Logistic Regression (hyperparameter optimized)\": OneVsRestClassifier(LogisticRegression(max_iter=1000, C= 1.0, penalty='l1', solver='liblinear')),\n",
    "    \"Balanced Logistic Regression (hyperparameter optimized)\": OneVsRestClassifier(LogisticRegression(class_weight='balanced', max_iter=1000, C= 0.1, penalty='l1', solver='liblinear')),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Random Forest (hyperparameter optimized)\": RandomForestClassifier(**rf_params)\n",
    "}\n",
    "\n",
    "label_names = merged_data[target_categories].columns.tolist()\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "\n",
    "    # Store metrics per fold, per label\n",
    "    fold_metrics = []\n",
    "\n",
    "    for train_idx, test_idx in stratifier.split(X, Y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "\n",
    "        Y_pred = Y_pred.toarray() if hasattr(Y_pred, 'toarray') else Y_pred\n",
    "\n",
    "        f1_micro = f1_score(Y_test, Y_pred, average='micro')  # or 'macro'\n",
    "        f1_macro = f1_score(Y_test, Y_pred, average='macro')\n",
    "        f1_scores.append({'f1_micro': f1_micro, 'f1_macro': f1_macro})\n",
    "        f1_scores_df = pd.DataFrame(f1_scores)\n",
    "\n",
    "        # Compute confusion matrices per label\n",
    "        cm = multilabel_confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "        rows = []\n",
    "\n",
    "        # Show false posotives and false negatives per label\n",
    "        for i, label in enumerate(label_names):\n",
    "            tn, fp, fn, tp = cm[i].ravel()\n",
    "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "            fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "            tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "            fold_metrics.append({\n",
    "                'Model': model_name,\n",
    "                'Fold': len(fold_metrics) // len(label_names) + 1,\n",
    "                'Label': label,\n",
    "                'F1_micro': f1_micro,\n",
    "                'F1_macro': f1_macro,\n",
    "                'FPR': fpr,\n",
    "                'FNR': fnr,\n",
    "                'TPR': tpr,\n",
    "                'TNR': tnr,\n",
    "                'TP': tp,\n",
    "                'FP': fp,\n",
    "                'FN': fn,\n",
    "                'TN': tn\n",
    "            })\n",
    "\n",
    "        # Convert per-fold metrics to DataFrame\n",
    "        fold_metrics_df = pd.DataFrame(fold_metrics)\n",
    "\n",
    "        # Aggregate over folds: mean per model and label\n",
    "        summary_df = fold_metrics_df.groupby(['Model', 'Label']).agg({\n",
    "            'F1_micro': 'mean',\n",
    "            'F1_macro': 'mean',\n",
    "            'FPR': 'mean',\n",
    "            'FNR': 'mean',\n",
    "            'TPR': 'mean',\n",
    "            'TNR': 'mean',\n",
    "        }).reset_index()\n",
    "\n",
    "        # Append summary to global results list\n",
    "        results.append(summary_df)\n",
    "\n",
    "        print(f\"Average F1 (micro): {np.mean(f1_scores_df.f1_micro):.4f}\")\n",
    "        print(f\"Average F1 (macro): {np.mean(f1_scores_df.f1_macro):.4f}\")\n",
    "        print(classification_report(Y_test, Y_pred, target_names=label_names))\n",
    "\n",
    "\n",
    "    # Combine all models into one DataFrame\n",
    "    final_results_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    final_results_df.to_csv('../out/model_performance_summary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for best parameters for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      " Best parameters (global) for base LR: {'estimator__C': 1.0, 'estimator__penalty': 'l1', 'estimator__solver': 'liblinear'}\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      " Best parameters (global) for balanced LR: {'estimator__C': 0.1, 'estimator__penalty': 'l1', 'estimator__solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Logistic Regression Hyperparameter Grid\n",
    "param_grid_lr = {\n",
    "    'estimator__C': [0.1, 1.0, 10.0],\n",
    "    'estimator__penalty': ['l1', 'l2'],\n",
    "    'estimator__solver': ['liblinear']  # Only solver supporting l1\n",
    "}\n",
    "\n",
    "# Model wrapper\n",
    "base_lr = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "balanced_lr = OneVsRestClassifier(LogisticRegression(class_weight='balanced', max_iter=1000))\n",
    "\n",
    "# For storing metrics\n",
    "# all_f1_scores = []\n",
    "\n",
    "# # Grid Search for base LR\n",
    "# for fold_idx, (train_idx, test_idx) in enumerate(stratifier.split(X, Y)):\n",
    "#     print(f\"\\n Fold {fold_idx + 1}/{n_splits}\")\n",
    "\n",
    "#     X_train, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "#     # Grid search\n",
    "#     grid_search = GridSearchCV(base_lr, param_grid_lr, cv=3, scoring='f1_micro', verbose=1, n_jobs=-1)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     print(f\"Best params for Base LR: {grid_search.best_params_}\")\n",
    "\n",
    "#     y_pred = best_model.predict(X_test)\n",
    "#     y_pred = y_pred.toarray() if hasattr(y_pred, \"toarray\") else y_pred\n",
    "\n",
    "#     f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "#     all_f1_scores.append(f1)\n",
    "\n",
    "#     print(f\"F1 Micro: {f1:.4f}\")\n",
    "#     print(classification_report(y_test, y_pred, target_names=label_names))\n",
    "\n",
    "# print(f\"\\n Average F1 Micro over all folds: {np.mean(all_f1_scores):.4f}\")\n",
    "\n",
    "# # Grid Search for base LR\n",
    "# for fold_idx, (train_idx, test_idx) in enumerate(stratifier.split(X, Y)):\n",
    "#     print(f\"\\n Fold {fold_idx + 1}/{n_splits}\")\n",
    "\n",
    "#     X_train, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "#     # Grid search\n",
    "#     grid_search = GridSearchCV(balanced_lr, param_grid_lr, cv=3, scoring='f1_micro', verbose=1, n_jobs=-1)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     print(f\"Best params for Balanced LR: {grid_search.best_params_}\")\n",
    "\n",
    "#     y_pred = best_model.predict(X_test)\n",
    "#     y_pred = y_pred.toarray() if hasattr(y_pred, \"toarray\") else y_pred\n",
    "\n",
    "#     f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "#     all_f1_scores.append(f1)\n",
    "\n",
    "#     print(f\"F1 Micro: {f1:.4f}\")\n",
    "#     print(classification_report(y_test, y_pred, target_names=label_names))\n",
    "\n",
    "# print(f\"\\n Average F1 Micro over all folds: {np.mean(all_f1_scores):.4f}\")\n",
    "\n",
    "grid_search_base_lr = GridSearchCV(\n",
    "    OneVsRestClassifier(LogisticRegression(max_iter=1000)),\n",
    "    param_grid_lr,\n",
    "    cv=5,\n",
    "    scoring='f1_micro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_base_lr.fit(X, Y)\n",
    "print(\" Best parameters (global) for base LR:\", grid_search_base_lr.best_params_)\n",
    "\n",
    "grid_search_balanced_lr = GridSearchCV(\n",
    "    OneVsRestClassifier(LogisticRegression(class_weight='balanced', max_iter=1000)),\n",
    "    param_grid_lr,\n",
    "    cv=5,\n",
    "    scoring='f1_micro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_balanced_lr.fit(X, Y)\n",
    "print(\" Best parameters (global) for balanced LR:\", grid_search_balanced_lr.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for Random Forest Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 126 candidates, totalling 1260 fits\n",
      " Best parameters (global) for Random Forest: {'estimator__max_depth': None, 'estimator__min_samples_split': 2, 'estimator__n_estimators': 75}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Random Forest Hyperparameter Grid\n",
    "param_grid_rf = {\n",
    "    'estimator__n_estimators': [50, 75, 100, 125, 150, 175, 200],\n",
    "    'estimator__max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'estimator__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Model wrapper\n",
    "base_rf = OneVsRestClassifier(RandomForestClassifier())\n",
    "\n",
    "# # For storing metrics\n",
    "# all_f1_scores = []\n",
    "\n",
    "# # Grid Search for base RF\n",
    "# for fold_idx, (train_idx, test_idx) in enumerate(stratifier.split(X, Y)):\n",
    "#     print(f\"\\n Fold {fold_idx + 1}/{n_splits}\")\n",
    "\n",
    "#     X_train, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "#     # Grid search\n",
    "#     grid_search = GridSearchCV(base_rf, param_grid_rf, cv=3, scoring='f1_micro', verbose=1, n_jobs=-1)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     print(f\"Best params for Random Forest: {grid_search.best_params_}\")\n",
    "\n",
    "#     y_pred = best_model.predict(X_test)\n",
    "#     y_pred = y_pred.toarray() if hasattr(y_pred, \"toarray\") else y_pred\n",
    "\n",
    "#     f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "#     all_f1_scores.append(f1)\n",
    "\n",
    "#     print(f\"F1 Micro: {f1:.4f}\")\n",
    "#     print(classification_report(y_test, y_pred, target_names=label_names))\n",
    "\n",
    "# print(f\"\\n Average F1 Micro over all folds: {np.mean(all_f1_scores):.4f}\")\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    OneVsRestClassifier(RandomForestClassifier()),\n",
    "    param_grid_rf,\n",
    "    cv=10,\n",
    "    scoring='f1_micro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_rf.fit(X, Y)\n",
    "print(\" Best parameters (global) for Random Forest:\", grid_search_rf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check class distribution for eventual additional parameters\n",
    "\n",
    "Since label distribution is unbalanced, the balanced_lr is included above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "PoliceReform: 0.2886\n",
      "Operations_PatrolsInterventions: 0.0830\n",
      "StateAdministration: 0.1204\n",
      "RefugeeAssistance: 0.0566\n",
      "ElectionAssistance: 0.0616\n",
      "LegalReform: 0.0709\n",
      "CivilSocietyAssistance: 0.0885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-28>:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution\n",
    "class_distribution = y.sum() / len(y)\n",
    "print(\"Class distribution:\")\n",
    "for i, category in enumerate(target_categories):\n",
    "    print(f\"{category}: {class_distribution[i]:.4f}\")\n",
    "\n",
    "# Use class_weight='balanced' for imbalanced data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
