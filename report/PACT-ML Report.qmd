---
title: "PACT-ML: Coding United Nation Peacekeeping Data from reports to the Secretary-General"
subtitle: "Using Bag-of-Words approaches and BERT models"
format:
  jasa-pdf:
    keep-tex: true  
    journal:
      blinded: false
  jasa-html: default
date: last-modified
author:
  - name: Felix Kube
    acknowledgements: | 
      Thanks for good advice during the semester, Chris and Killian. Also all the peers I talked to while tackling this group project alone :) it was not so alone after all.
    affiliations:
      - name: Hertie School
        department: Data Science Lab
abstract: |
  The Peacekeeping Activity Dataset (PACT) is the first of its kind data collection to shine light on what peacekeepers actually implement while deployed. In the past, many projects have looked towards mandates to study how specific tasks and mission success are related. PACT used report data from the mission heads to the Secretary-General of the UN to code up to 39 categories of task implementation on six different engagement levels. This project, PACT-ML, aims to extend the data collections of PACT 1.0 (@Blair2022) and PACT 2.0 (@PACT2, @Otto2024) by using selected reports of PACT 2.0 to examine the application of Machine Learning / Natural Language Processing techniques to automatically code this sort of data from the reports.
keywords:
  - Machine Learning
  - Natural Language Processing
  - United Nations Peacekeeping
  - BERT
  - roBERTa
bibliography: bibliography.bib  
---

## Notes

- ONUCA reports left out due to super old report format and issues in preprocessing.
- UNMIBH and UNTAET reports left out due to 2 column layout
- maybe make cutoff at the introduction of the newer format?
- leave out cross-country missions due to problems in data parsing and different language format (only coded on state-level, when that state's name is explicitely mentioned)
  - could also preprocess data and merge those states back together


## Introduction {#sec-intro}



- Note that figures and tables (such as @tbl-one) should appear in the paper, not at the end or in separate files.

Motivation: Feasibility test to use data of this kind to automatically code newly written reports.

Background on the research project itself



## Data {#sec-data}

- Describe PACT data, describe report formats, reference the PACT/PEMA codebooks with same coding categories for all reports since 1989

The UNPKOs included are: MINUGUA, MINUJUSTH, MINUSTAH, MIPONUH, ONUCA, ONUSAL, UNCPSG, UNCRO, UNMIBH, UNMIH, UNMIK, UNMISET, UNMIT, UNMOP, UNMOT, UNOMIG, UNPREDEP, UNPROFOR, UNSMIH, UNTAC, UNTAES, UNTAET and UNTMIH.

Some old reports (e.g. ONUCA) were left out due to issues in parsing, because the official UN documents are just a bad scan of typewritten reports. UNMIBH and UNTAET reports were left out due to their two-column layout. We also decided for the exclusion of cross-country reports (missions active in more than one country) due to issues with the systematic difference in language to describe activities and the adjacent codings, which are sometimes at the country level and sometimes at the report level. For identifying the reports to parse and use for the model, we used 'PyMuPDF' and selected based on some characteristics from the file, like first page margins. The reports were then clustered using kNN, and the second cluster included the report types that could be parsed. There were 133 reports identified, which was deemed sufficient for the scope of this term paper. If we assume that the content of the reports and the features of the layout used for clustering do not coincide, the subsetting should not interfere with our results systematically. In summary, the data corresponds with all post-2000 UN reports included in the PACT 2.0 data set, which in academical language coincides with the third and fourth generations of UN Peacekeeping Missions.

### Validity checks

To be able to use our models on the data, we need to identify which paragraph text belongs to the specific coding in the PACT 2.0 data set. We take the reported paragraph, from which the manual coders at the University of Uppsala made their judgement, as ground truth. We do this because it is highly unlikely that mistakes happened, as each paragraph as per the UN reporting scheme carries its paragraph number at the beginning, and coders had to mark the relevant sentences within the PDFs before adding them to the database.[^1]

The reports were then parsed. Since the Annexes were not as expected in a separate file, but included in the main report, the numbering at times was not consecutive, but started again at '1.' in the middle of the parsed paragraphs. To sucessfully eliminate the paragraphs that were extracted from the Annexes, the paragraphs number at the beginning of each paragraph needed to increase strictly monotonically. Reports failed the parsing pipeline and were not added to the training data if the extracted paragraphs were 10% more or less than the expected paragraphs according to the PACT data. This ensures that the good data quality of parsed reports.[^2]

[^1]: On a side note, this fine-grained data which would allow us to train our models beyond the scope here, is only available for 143 out of 470 total reports. Therefore, pre-trained BERT-class models were used.
[^2]: Two more reports failed the parsing pipeline due to incompatible PDF formats and were therefore deleted from the pre-selection. While the reports were the standard page size of 612 x 792 units like the others, the bounding box to extract text failed for these reports.

| one  | two  | three | four | five |
|------|------|-------|------|------|
| 1.23 | 3.45 | 5.00  | 1.21 | 3.41 |
| 1.23 | 3.45 | 5.00  | 1.21 | 3.42 |
| 1.23 | 3.45 | 5.00  | 1.21 | 3.43 |

: Included reports in PACT 2.0, after pre-selection and after parsing. {#tbl-included}

Above table (@tbl-included) shows the number of reports per UNPKO in PACT 2.0, as well as shares of the reports that made it through pre-selection and parsing.



Describe process how the data is parsed to extract the paragraphs


## Methods {#sec-meth}

Don't take any of these section titles seriously.
They're just for illustration.

## Results {#sec-results}


## Conclusion {#sec-conc}


## Supplementary Material {.supplementary}

Title:

:   Brief description. (file type)





 
